{
  "agent_id": "coder2",
  "task_id": "task_3",
  "files": [
    {
      "name": "memory.py",
      "purpose": "Experience replay and memory",
      "priority": "medium"
    },
    {
      "name": "reward_system.py",
      "purpose": "Reward calculation and shaping",
      "priority": "medium"
    }
  ],
  "project_info": {
    "project_name": "enhanced_cs.CV_2508.18265v1_InternVL35_Advancing_Open_Source_Multimodal_Mode",
    "project_type": "agent",
    "description": "Enhanced AI project based on cs.CV_2508.18265v1_InternVL35-Advancing-Open-Source-Multimodal-Mode with content analysis. Detected project type: agent (confidence score: 11 matches).",
    "key_algorithms": [
      "Consistency",
      "Sft",
      "Scaling",
      "Rlhf",
      "Representation",
      "Machine",
      "5-4B",
      "Specialized",
      "Multimodal",
      "Reference"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "\n--- chunk_2.txt ---\nPDF: cs.CV_2508.18265v1_InternVL35-Advancing-Open-Source-Multimodal-Mode.pdf\nChunk: 2/2\n==================================================\n\n--- Page 29 ---\n[78] Yujie Lu, Dongfu Jiang, Wenhu Chen, William Yang Wang, Yejin Choi, and Bill Yuchen Lin. Wildvision: Evaluating\nvision-language models in the wild with human preferences. arXiv preprint arXiv:2406.11069 , 2024. 8, 13\n[79] Gen Luo, Wenhan Dou, Wenhao Li, Zhaokai Wang, Xue Yang, Changyao Tian, Hao Li, Weiyun Wang, Wenhai Wang,\nXizhou Zhu, Yu Qiao, and Jifeng Dai. Mono-internvl-1.5: Towards cheaper and faster monolithic multimodal large\nlanguage models. arXiv preprint arXiv:2507.12566 , 2025. 2\n[80] Gen Luo, Xue Yang, Wenhan Dou, Zhaokai Wang, Jifeng Dai, Yu Qiao, and Xizhou Zhu. Mono-internvl: Pushing the\nboundaries of monolithic multimodal large language models with endogenous visual pre-training. arXiv preprint\narXiv:2410.08202 , 2024. 2, 10\n[81] Gen Luo, Yiyi Zhou, Yuxin Zhang, Xiawu Zheng, Xiaoshuai Sun, and Rongrong Ji. Feast your eyes: Mixture-of-\nresolution adaptation for multimodal large language models. arXiv preprint arXiv:2403.03003 , 2024. 2\n[82] Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng,\nJiao Sun, et al. Improve mathematical reasoning in language models by automated process supervision. arXiv preprint\narXiv:2406.06592 , 2, 2024. 1, 7\n[83] Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun\nZhang, Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with a 1.5b model by scaling\nrl, 2025. Notion Blog. 7\n[84] MAA. American invitational mathematics examination - aime. https://maa.org/\nmaa-invitational-competitions/ , 2024. 1, 2, 8, 20\n[85] MAA. American invitational mathematics examination - aime. https://maa.org/\nmaa-invitational-competitions/ , 2025. 2, 8, 20\n[86] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy. Generation\nand comprehension of unambiguous object descriptions. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 11\u201320, 2016. 15\n[87] Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: A benchmark for question\nanswering about charts with visual and logical reasoning. In Proceedings of the Annual Meeting of the Association\nfor Computational Linguistics , pages 2263\u20132279, 2022. 11, 12\n[88] Minesh Mathew, Viraj Bagal, Rub\u00e8n Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa.\nInProceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision , pages 1697\u20131706, 2022. 11,\n12\n[89] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: A dataset for vqa on document images. In\nProceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision , pages 2200\u20132209, 2021. 11, 12\n[90] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang,\nJunjun He, Kaipeng Zhang, et al. Mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement\nlearning. arXiv preprint arXiv:2503.07365 , 2025. 1, 7\n[91] Fanqing Meng, Jin Wang, Chuanhao Li, Quanfeng Lu, Hao Tian, Jiaqi Liao, Xizhou Zhu, Jifeng Dai, Yu Qiao, Ping\nLuo, et al. Mmiu: Multimodal multi-image understanding for evaluating large vision-language models. arXiv preprint\narXiv:2408.02718 , 2024. 12, 13\n[92] Meta AI. Llama 4 maverick 17b-128e (model card). Hugging Face, 2025. Released Apr 5, 2025; MoE with 17B\nactivated / 400B total; native multimodality; knowledge cutoff Aug 2024. 21\n[93] Meta AI. Llama 4 scout 17b-16e (model card). Hugging Face, 2025. Model release date: Apr 5, 2025; MoE 17B\nactive / 109B total; 10M context. 21\n[94] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy\nLiang, Emmanuel Cand\u00e8s, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393 ,\n2025. 4\n[95] OpenAI. Gpt-4v(ision) system card. https://cdn.openai.com/papers/GPTV_System_Card.pdf ,\n2023. 12, 13, 14, 16, 17\n[96] OpenAI. Computer-using agent: Introducing a universal interface for ai to interact with the digital world. 2025. 18\n[97] OpenAI. Gpt-4o system card. https://openai.com/index/gpt-4o-system-card/ , 2025. 12, 13, 14,\n16, 17, 18, 19, 20, 21\n29\n\n--- Page 30 ---\n[98] OpenAI. Gpt-5 system card. https://cdn.openai.com/pdf/\n8124a3ce-ab78-4f06-96eb-49ea29ffb52f/gpt5-system-card-aug7.pdf , 2025. 2, 3, 10,\n11, 19, 20\n[99] OpenAI. Introducing gpt-oss. https://openai.com/index/introducing-gpt-oss/, 2025. 3\n[100] Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue, Shanglin Lei,\nZhe Wei, Miaoxuan Zhang, et al. We-math: Does your large multimodal model achieve human-like mathematical\nreasoning? arXiv preprint arXiv:2407.01284 , 2024. 2, 8, 10, 11\n[101] Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li,\nShijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326 ,\n2025. 1, 18\n[102] Zeju Qiu, Weiyang Liu, Haiwen Feng, Zhen Liu, Tim Z Xiao, Katherine M Collins, Joshua B Tenenbaum, Adrian\nWeller, Michael J Black, and Bernhard Sch\u00f6lkopf. Can large language models understand symbolic graphics programs?\narXiv preprint arXiv:2408.08313 , 2024. 2, 10, 19, 20\n[103] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are\nunsupervised multitask learners. OpenAI blog , 1(8):9, 2019. 4\n[104] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct\npreference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing\nSystems , 36, 2024. 5\n[105] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu\nSoricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding\nacross millions of tokens of context. arXiv preprint arXiv:2403.05530 , 2024. 12, 13, 14, 17\n[106] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael,\nand Samuel R Bowman. Gpqa: A graduate-level google-proof q&a benchmark. In First Conference on Language\nModeling , 2024. 1, 2, 10\n[107] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd\nschema challenge at scale. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 34, pages\n8732\u20138740, 2020. 20\n[108] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization\nalgorithms. arXiv preprint arXiv:1707.06347 , 2017. 5\n[109] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang,\nYK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv\npreprint arXiv:2402.03300 , 2024. 1, 5\n[110] Wei Shen, Jiangbo Pei, Yi Peng, Xuchen Song, Yang Liu, Jian Peng, Haofeng Sun, Yunzhuo Hao, Peiyu Wang, and\nYahui Zhou. Skywork-r1v3 technical report. arXiv preprint arXiv:2507.06167 , 2025. 11\n[111] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and\nChuan Wu. Hybridflow: A flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256 , 2024. 7\n[112] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus\nRohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , pages 8317\u20138326, 2019. 11, 12\n[113] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more\neffective than scaling model parameters. arXiv preprint arXiv:2408.03314 , 2024. 7\n[114] Hai-Long Sun, Da-Wei Zhou, Yang Li, Shiyin Lu, Chao Yi, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang,\nDe-Chuan Zhan, et al. Parrot: Multilingual visual instruction tuning. arXiv preprint arXiv:2406.02539 , 2024. 16\n[115] Kai Sun, Dian Yu, Dong Yu, and Claire Cardie. Investigating prior knowledge for challenging chinese machine\nreading comprehension. Transactions of the Association for Computational Linguistics , 8:141\u2013155, 2020. 20\n[116] Qiushi Sun, Zhoumianze Liu, Chang Ma, Zichen Ding, Fangzhi Xu, Zhangyue Yin, Haiteng Zhao, Zhenyu Wu,\nKanzhi Cheng, Zhaoyang Liu, et al. Scienceboard: Evaluating multimodal autonomous agents in realistic scientific\nworkflows. arXiv preprint arXiv:2505.19897 , 2025. 1\n[117] Mirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha\nChowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can\nsolve them. arXiv preprint arXiv:2210.09261 , 2022. 20\n30\n\n--- Page 31 ---\n[118] Suzhongling, Rong Fu, Weihan Cao, Jianfei Gao, Minxi Jin, PeiZhilin, and Hui Wang. TMA-adaptive FP8 grouped\nGEMM: Eliminating padding requirements in low-precision training and inference on hopper. In ES-FoMo III: 3rd\nWorkshop on Efficient Systems for Foundation Models , 2025. 7\n[119] Jingqun Tang, Qi Liu, Yongjie Ye, Jinghui Lu, Shu Wei, Chunhui Lin, Wanqing Li, Mohamad Fitri Faiz Bin Mahmood,\nHao Feng, Zhen Zhao, et al. Mtvqa: Benchmarking multilingual text-centric visual question answering. arXiv preprint\narXiv:2405.11985 , 2024. 8, 16\n[120] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan\nSchalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint\narXiv:2312.11805 , 2023. 16\n[121] Gemini Robotics Team, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas,\nTravis Armstrong, Ashwin Balakrishna, Robert Baruch, Maria Bauza, Michiel Blokzijl, et al. Gemini robotics:\nBringing ai into the physical world. arXiv preprint arXiv:2503.20020 , 2025. 2, 10, 19\n[122] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana\nMatejovicova, Alexandre Ram\u00e9, Morgane Rivi\u00e8re, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786 ,\n2025. 11, 12, 13, 14, 16, 18, 19, 20, 21\n[123] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre,\nMorgane Rivi\u00e8re, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and\ntechnology. arXiv preprint arXiv:2403.08295 , 2024. 20\n[124] InternLM Team. Internlm: A multilingual language model with progressively enhanced capabilities. https:\n//github.com/InternLM/InternLM , 2023. 4\n[125] Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang,\nChenzhuang Du, Chu Wei, et al. Kimi-vl technical report. arXiv preprint arXiv:2504.07491 , 2025. 10, 11, 12, 13, 14,\n16, 17, 19, 20, 21\n[126] Kwai Keye Team, Biao Yang, Bin Wen, Changyi Liu, Chenglong Chu, Chengru Song, Chongling Rao, Chuan Yi,\nDa Li, Dunju Zang, et al. Kwai keye-vl technical report. arXiv preprint arXiv:2507.01949 , 2025. 1, 3, 11, 12, 13, 14,\n16, 17, 19, 20, 21\n[127] Qwen Team. Qvq: To see the world with wisdom, December 2024. 11\n[128] Qwen Team. Qwen2.5: A party of foundation models. https://qwenlm.github.io/blog/qwen2.5/ ,\nSeptember 2024. 1\n[129] StepFun Team. Step3: Cost-effective multimodal intelligence. 2, 10, 11, 19, 20, 21\n[130] Philippe Tillet, Hsiang-Tsung Kung, and David Cox. Triton: an intermediate language and compiler for tiled neural\nnetwork computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and\nProgramming Languages , pages 10\u201319, 2019. 7\n[131] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang,\nShusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: A fully open, vision-centric exploration of multimodal\nllms. arXiv preprint arXiv:2406.16860 , 2024. 12, 13, 14\n[132] Fei Wang, Xingyu Fu, James Y Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan\nZhou, Kai Zhang, et al. Muirbench: A comprehensive benchmark for robust multi-image understanding. arXiv\npreprint arXiv:2406.09411 , 2024. 12, 13\n[133] Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vl-rethinker: Incentivizing\nself-reflection of vision-language models with reinforcement learning. arXiv preprint arXiv:2504.08837 , 2025. 1\n[134] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical\nreasoning with math-vision dataset. arXiv preprint arXiv:2402.14804 , 2024. 1, 2, 10, 11\n[135] Peijie Wang, Zhong-Zhi Li, Fei Yin, Dekang Ran, and Cheng-Lin Liu. Mv-math: Evaluating multimodal math\nreasoning in multi-visual contexts. In Proceedings of the Computer Vision and Pattern Recognition Conference , pages\n19541\u201319551, 2025. 7\n[136] Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd:\nVerify and reinforce llms step-by-step without human annotations. arXiv preprint arXiv:2312.08935 , 2023. 1\n[137] Peiyu Wang, Yichen Wei, Yi Peng, Xiaokun Wang, Weijie Qiu, Wei Shen, Tianyidan Xie, Jiangbo Pei, Jianhao\nZhang, Yunzhuo Hao, et al. Skywork r1v2: Multimodal hybrid reinforcement learning for reasoning. arXiv preprint\narXiv:2504.16656 , 2025. 1\n31\n\n--- Page 32 ---\n[138] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin\nGe, et al. Qwen2-vl: Enhancing vision-language model\u2019s perception of the world at any resolution. arXiv preprint\narXiv:2409.12191 , 2024. 3, 11, 12, 13, 14, 15, 16, 17\n[139] Peng Wang, Shijie Wang, Junyang Lin, Shuai Bai, Xiaohuan Zhou, Jingren Zhou, Xinggang Wang, and Chang Zhou.\nOne-peace: Exploring one general representation model toward unlimited modalities. arXiv:2305.11172 , 2023. 15\n[140] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan\nSong, et al. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079 , 2023. 15\n[141] Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Jinguo Zhu, Xizhou Zhu, Lewei Lu,\nYu Qiao, and Jifeng Dai. Enhancing the reasoning ability of multimodal large language models via mixed preference\noptimization. arXiv preprint arXiv:2411.10442 , 2024. 2, 3, 5, 7\n[142] Weiyun Wang, Zhangwei Gao, Lianjie Chen, Zhe Chen, Jinguo Zhu, Xiangyu Zhao, Yangzhou Liu, Yue Cao,\nShenglong Ye, Xizhou Zhu, et al. Visualprm: An effective process reward model for multimodal reasoning. arXiv\npreprint arXiv:2503.10291 , 2025. 1, 7, 11\n[143] Weiyun Wang, Yiming Ren, Haowen Luo, Tiantong Li, Chenxiang Yan, Zhe Chen, Wenhai Wang, Qingyun Li, Lewei\nLu, Xizhou Zhu, et al. The all-seeing project v2: Towards general relation comprehension of the open world. arXiv\npreprint arXiv:2402.19474 , 2024. 14, 15\n[144] Xuehui Wang, Zhenyu Wu, JingJing Xie, Zichen Ding, Bowen Yang, Zehao Li, Zhaoyang Liu, Qingyun Li, Xuan\nDong, Zhe Chen, Weiyun Wang, Xiangyu Zhao, Jixuan Chen, Haodong Duan, Tianbao Xie, Shiqian Su, Chenyu\nYang, Yue Yu, Yuan Huang, Yiqian Liu, Xiao Zhang, Xiangyu Yue, Weijie Su, Xizhou Zhu, Wei Shen, Jifeng Dai,\nand Wenhai Wang. Mmbench-gui: Hierarchical multi-platform evaluation framework for gui agents. arXiv preprint\narXiv:2507.19478 , 2025. 18\n[145] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran\nArulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: A more robust and challenging multi-task language understanding\nbenchmark. arXiv preprint arXiv:2406.01574 , 2024. 2\n[146] Zhaokai Wang, Xizhou Zhu, Xue Yang, Gen Luo, Hao Li, Changyao Tian, Wenhan Dou, Junqi Ge, Lewei Lu, Yu Qiao,\nand Jifeng Dai. Parameter-inverted image pyramid networks for visual perception and multimodal understanding.\nIEEE transactions on pattern analysis and machine intelligence , 2025. 2\n[147] Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu,\nSadhika Malladi, et al. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. arXiv preprint\narXiv:2406.18521 , 2024. 11, 12\n[148] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: A benchmark for long-context interleaved\nvideo-language understanding. arXiv preprint arXiv:2407.15754 , 2024. 8, 17\n[149] Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding,\nLiheng Chen, Paul Pu Liang, et al. Os-atlas: A foundation action model for generalist gui agents. arXiv preprint\narXiv:2410.23218 , 2024. 10, 18\n[150] xAI. Grok 3 beta \u2014 the age of reasoning agents. https://x.ai/news/grok-3, 2025. 21\n[151] Zhiheng Xi, Guanyu Li, Yutao Fan, Honglin Guo, Yufang Liu, Xiaoran Fan, Jiaqi Liu, Jingchao Ding, Wangmeng\nZuo, Zhenfei Yin, et al. Bmmr: A large-scale bilingual multimodal multi-discipline reasoning dataset. arXiv preprint\narXiv:2507.03483 , 2025. 1\n[152] Yijia Xiao, Edward Sun, Tianyu Liu, and Wei Wang. Logicvista: Multimodal llm logical reasoning benchmark in\nvisual contexts. arXiv preprint arXiv:2407.04973 , 2024. 2, 8, 10, 11\n[153] LLM-Core-Team Xiaomi. Mimo-vl technical report, 2025. 3, 11, 18, 19\n[154] Tianbao Xie, Jiaqi Deng, Xiaochuan Li, Junlin Yang, Haoyuan Wu, Jixuan Chen, Wenjing Hu, Xinyuan Wang,\nYuhui Xu, Zekun Wang, Yiheng Xu, Junli Wang, Doyen Sahoo, Tao Yu, and Caiming Xiong. Scaling computer-use\ngrounding via user interface decomposition and synthesis, 2025. 18\n[155] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh J Hua, Zhoujun Cheng,\nDongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer\nenvironments. Advances in Neural Information Processing Systems , 37:52040\u201352094, 2024. 1, 10, 18\n[156] Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming\nXiong. Aguvis: Unified pure vision agents for autonomous gui interaction. arXiv preprint arXiv:2412.04454 , 2024.\n18\n32\n\n--- Page 33 ---\n[157] B. Yan, Yi Jiang, Jiannan Wu, D. Wang, Ping Luo, Zehuan Yuan, and Huchuan Lu. Universal instance perception\nas object discovery and retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , 2023. 15\n[158] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen\nHuang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388 , 2025. 3\n[159] Chenyu Yang, Shiqian Su, Shi Liu, Xuan Dong, Yue Yu, Weijie Su, Xuehui Wang, Zhaoyang Liu, Jinguo Zhu, Hao Li,\net al. Zerogui: Automating online gui learning at zero human cost. arXiv preprint arXiv:2505.23762 , 2025. 1\n[160] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in Space: How\nMultimodal Large Language Models See, Remember and Recall Spaces. arXiv preprint arXiv:2412.14171 , 2024. 1,\n2, 10, 19\n[161] Yan Yang, Dongxu Li, Yutong Dai, Yuhao Yang, Ziyang Luo, Zirui Zhao, Zhiyuan Hu, Junzhe Huang, Amrita Saha,\nZeyuan Chen, et al. Gta1: Gui test-time scaling agent. arXiv preprint arXiv:2507.05791 , 2025. 18\n[162] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao,\nMinfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization.\narXiv preprint arXiv:2503.10615 , 2025. 7\n[163] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui\nHe, et al. Minicpm-v: A gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800 , 2024. 3, 11, 12, 13, 14,\n16, 17, 19\n[164] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou.\nmplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. arXiv preprint\narXiv:2311.04257 , 2023. 16\n[165] Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo\nLiu, Jiayi Lei, Quanfeng Lu, Runjian Chen, Peng Xu, Renrui Zhang, Haozhe Zhang, Peng Gao, Yali Wang, Yu Qiao,\nPing Luo, Kaipeng Zhang, and Wenqi Shao. Mmt-bench: A comprehensive multimodal benchmark for evaluating\nlarge vision-language models towards multitask agi. arXiv preprint arXiv:2404.16006 , 2024. 12, 13\n[166] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu,\nLingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476 ,\n2025. 5\n[167] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.\nMm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490 , 2023. 2,\n8, 14\n[168] Ya-Qi Yu, Minghui Liao, Jiwen Zhang, and Jihao Wu. Texthawk2: A large vision-language model excels in bilingual\nocr and grounding with 16x fewer tokens. arXiv preprint arXiv:2410.05261 , 2024. 15\n[169] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming\nRen, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for\nexpert agi. arXiv preprint arXiv:2311.16502 , 2023. 1, 2, 8, 10, 11\n[170] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement\nlearning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837 , 2025.\n1\n[171] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish\nyour sentence? In Proceedings of the Annual Meeting of the Association for Computational Linguistics , pages\n4791\u20134800, 2019. 20\n[172] Haotian Zhang, Haoxuan You, Philipp Dufter, Bowen Zhang, Chen Chen, Hong-You Chen, Tsu-Jui Fu, William Yang\nWang, Shih-Fu Chang, Zhe Gan, et al. Ferret-v2: An improved baseline for referring and grounding with large\nlanguage models. arXiv preprint arXiv:2404.07973 , 2024. 15\n[173] Junlei Zhang, Zichen Ding, Chang Ma, Zijie Chen, Qiushi Sun, Zhenzhong Lan, and Junxian He. Breaking the data\nbarrier \u2013 building gui agents through task generalization. arXiv preprint arXiv:2504.10127 , 2025. 1\n[174] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei\nChang, Peng Gao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?\narXiv preprint arXiv:2403.14624 , 2024. 2, 8, 10, 11\n33\n\n--- Page 34 ---\n[175] Tianyu Zhang, Suyuchen Wang, Lu Li, Ge Zhang, Perouz Taslakian, Sai Rajeswar, Jie Fu, Bang Liu, and Yoshua\nBengio. Vcr: Visual caption restoration. arXiv preprint arXiv:2406.06462 , 2024. 11, 12\n[176] Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, and Xipeng Qiu. Evaluating the performance of\nlarge language models on gaokao benchmark. arXiv preprint arXiv:2305.12474 , 2023. 1, 2, 10, 20\n[177] Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang,\nQingsong Wen, Zhang Zhang, et al. Mme-realworld: Could your multimodal llm challenge high-resolution real-world\nscenarios that are difficult for humans? arXiv preprint arXiv:2408.13257 , 2024. 2, 8, 13\n[178] Yichi Zhang, Siyuan Zhang, Yao Huang, Zeyu Xia, Zhengwei Fang, Xiao Yang, Ranjie Duan, Dong Yan, Yinpeng\nDong, and Jun Zhu. Stair: Improving safety alignment with introspective reasoning. arXiv preprint arXiv:2502.02384 ,\n2025. 7\n[179] Yipeng Zhang, Yifan Liu, Zonghao Guo, Yidan Zhang, Xuesong Yang, Chi Chen, Jun Song, Bo Zheng, Yuan Yao,\nZhiyuan Liu, et al. Llava-uhd v2: an mllm integrating high-resolution feature pyramid via hierarchical window\ntransformer. arXiv preprint arXiv:2412.13871 , 2024. 2\n[180] Bingchen Zhao, Yongshuo Zong, Letian Zhang, and Timothy Hospedales. Benchmarking multi-image understand-\ning in vision and language models: Perception, knowledge, reasoning, and multi-hop reasoning. arXiv preprint\narXiv:2406.12742 , 2024. 12, 13\n[181] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri,\nMyle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint\narXiv:2304.11277 , 2023. 7\n[182] Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men,\nAn Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071 , 2025. 1, 2, 5\n[183] Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao\nZhou, et al. Secrets of rlhf in large language models part i: Ppo. arXiv preprint arXiv:2307.04964 , 2023. 5\n[184] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou.\nInstruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911 , 2023. 2, 10\n[185] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang,\nand Zheng Liu. Mlvu: A comprehensive benchmark for multi-task long video understanding. arXiv preprint\narXiv:2406.04264 , 2024. 8, 17\n[186] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su,\nJie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models.\narXiv preprint arXiv:2504.10479 , 2025. 2, 3, 4, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21\n[187] Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Y Wu, Yukun Li, Huazuo Gao, Shirong\nMa, et al. Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence. arXiv preprint\narXiv:2406.11931 , 2024. 20\n[188] Chengke Zou, Xingang Guo, Rui Yang, Junyu Zhang, Bin Hu, and Huan Zhang. Dynamath: A dynamic visual bench-\nmark for evaluating mathematical reasoning robustness of vision language models. arXiv preprint arXiv:2411.00836 ,\n2024. 2, 8, 10, 11\n34\n",
  "project_dir": "artifacts/projects/enhanced_cs.CV_2508.18265v1_InternVL35_Advancing_Open_Source_Multimodal_Mode",
  "communication_dir": "artifacts/projects/enhanced_cs.CV_2508.18265v1_InternVL35_Advancing_Open_Source_Multimodal_Mode/.agent_comm",
  "assigned_at": "2025-08-26T21:07:33.248590",
  "status": "assigned"
}