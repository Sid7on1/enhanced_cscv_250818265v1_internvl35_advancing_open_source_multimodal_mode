{
  "agent_id": "coder1",
  "task_id": "task_6",
  "files": [
    {
      "name": "training.py",
      "purpose": "Agent training pipeline",
      "priority": "high"
    },
    {
      "name": "policy.py",
      "purpose": "Policy network implementation",
      "priority": "high"
    }
  ],
  "project_info": {
    "project_name": "enhanced_cs.CV_2508.18265v1_InternVL35_Advancing_Open_Source_Multimodal_Mode",
    "project_type": "agent",
    "description": "Enhanced AI project based on cs.CV_2508.18265v1_InternVL35-Advancing-Open-Source-Multimodal-Mode with content analysis. Detected project type: agent (confidence score: 11 matches).",
    "key_algorithms": [
      "Consistency",
      "Sft",
      "Scaling",
      "Rlhf",
      "Representation",
      "Machine",
      "5-4B",
      "Specialized",
      "Multimodal",
      "Reference"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "\n--- chunk_1.txt ---\nPDF: cs.CV_2508.18265v1_InternVL35-Advancing-Open-Source-Multimodal-Mode.pdf\nChunk: 1/2\n==================================================\n\n--- Page 1 ---\nInternVL3.5: Advancing Open-Source Multimodal Models\nin Versatility, Reasoning, and Efficiency\nWeiyun Wang\u2217, Zhangwei Gao\u2217, Lixin Gu\u2217, Hengjun Pu\u2217, Long Cui\u2217, Xingguang Wei\u2217, Zhaoyang Liu\u2217, Linglin Jing\u2217,\nShenglong Ye\u2217, Jie Shao\u2217, Zhaokai Wang\u2217, Zhe Chen\u2217, Hongjie Zhang, Ganlin Yang, Haomin Wang, Qi Wei, Jinhui Yin,\nWenhao Li, Erfei Cui, Guanzhou Chen, Zichen Ding, Changyao Tian, Zhenyu Wu, Jingjing Xie, Zehao Li, Bowen Yang,\nYuchen Duan, Xuehui Wang, Songze Li, Xiangyu Zhao, Haodong Duan, Nianchen Deng, Bin Fu, Yinan He, Yi Wang,\nConghui He, Botian Shi, Junjun He, Yingtong Xiong, Han Lv, Lijun Wu, Wenqi Shao, Kaipeng Zhang, Huipeng Deng,\nBiqing Qi, Jiaye Ge, Qipeng Guo, Wenwei Zhang, Wanli Ouyang, Limin Wang, Min Dou, Xizhou Zhu, Tong Lu,\nDahua Lin, Jifeng Dai, Weijie Su, Bowen ZhouB, Kai ChenB, Yu QiaoB, Wenhai WangB\u2020, Gen LuoB\u2020\nInternVL Team, Shanghai AI Laboratory\nCode: https://github.com/OpenGVLab/InternVL\nModel: https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B\nAbstract\nWe introduce InternVL 3.5 , a new family of open-source multimodal models that significantly\nadvances versatility, reasoning capability, and inference efficiency along the InternVL series.\nA key innovation is the Cascade Reinforcement Learning (Cascade RL) framework, which\nenhances reasoning through a two-stage process: offline RL for stable convergence and\nonline RL for refined alignment. This coarse-to-fine training strategy leads to substantial\nimprovements on downstream reasoning tasks, e.g., MMMU and MathVista. To optimize\nefficiency, we propose a Visual Resolution Router (ViR) that dynamically adjusts the resolution\nof visual tokens without compromising performance. Coupled with ViR, our Decoupled Vision-\nLanguage Deployment (DvD) strategy separates the vision encoder and language model across\ndifferent GPUs, effectively balancing computational load. These contributions collectively\nenable InternVL3.5 to achieve up to a +16.0% gain in overall reasoning performance and a\n4.05\u00d7inference speedup compared to its predecessor, i.e., InternVL3. In addition, InternVL3.5\nsupports novel capabilities such as GUI interaction and embodied agency. Notably, our\nlargest model, i.e., InternVL3.5-241B-A28B, attains state-of-the-art results among open-\nsource MLLMs across general multimodal, reasoning, text, and agentic tasks\u2014narrowing\nthe performance gap with leading commercial models like GPT-5. All models and code are\npublicly released.\n1 Introduction\nThe recent trend of Multimodal Large Language Models (MLLMs) [ 46,126,128] has gone beyond simple\nmultimodal understanding and gradually focused on more general, complex, and realistic tasks such as text-\nrelated tasks [ 12,44,49,84,106,176], reasoning tasks [ 43,76,134,160,169] and agentic tasks [ 38,48,50,101,\n116,155,159,160,173]. In these aspects, commercial models have created huge gaps with current open-source\nmodels, as shown in Table 2. Thereby, recent open-source efforts [ 46,90,126,137] aim to explore advanced\nreinforcement learning (RL) methods to mitigate the gap and pursue higher multimodal intelligence. However,\ndespite much effort in RL algorithms [ 23,24,90,109,133,151,170,182] and verifiers [ 82,136,142], a stable,\neffective, and scalable reinforcement learning framework for MLLMs still remains an open problem in the\ncommunity. Furthermore, the growth of multimodal capabilities, e.g., long visual context and high-resolution\n* equal contribution; Bcorresponding authors; \u2020technical leaders.arXiv:2508.18265v1  [cs.CV]  25 Aug 2025\n\n--- Page 2 ---\nGPT-5\nGemini-2.5 Pro\nInternVL3.5-241B-A28BGLM-4.5V-106B-A12BInternVL3.5-38B\nInternVL3.5-30B-A3BInternVL3.5-14BStep-3-321B-A38B Claude 3.7 SonnetInternVL3.5-8B\nInternVL3.5-20B-A4BMiMo-VL-RL-7BGLM-4.1V-9BInternVL3.5-4BQwen2.5-VL-72B\nKimi-VL-2506-16B-A3BGPT-4o\nKeye-VL-8B\nQwen2.5-VL-32BInternVL3.5-2B Gemma-3-27BQwen2.5-VL-7BMiniCPM-V-4-4BGemma-3-12BQwen2.5-VL-3B InternVL3.5-1B\n76.5\n75.1\n72.6\n70.4 70.4\n67.666.965.965.3 65.264.463.6\n62.2\n60.8\n59.458.8\n56.9 56.5\n55.053.8\n50.9 50.7\n47.346.4\n44.9\n42.7Figure 1: Comparison between InternVL3.5 and leading MLLMs in general capabilities. Hatched bars\nrepresent closed-source commercial models. We report average scores on a set of multimodal general, reasoning,\ntext, and agentic benchmarks: MMBench v1.1 (en) [ 71], MMStar [ 11], BLINK [ 36], HallusionBench [ 41],\nAI2D [ 55], OCRBench [ 72], MMVet [ 167], MME-RealWorld (en) [ 177], MVBench [ 63], VideoMME [ 35],\nMMMU [ 169], MathVista [ 76], MathVision [ 134], MathVerse [ 174], DynaMath [ 188], WeMath [ 100], Log-\nicVista [ 152], MATH500 [ 45], AIME24 [ 84], AIME25 [ 85], GPQA [ 106], MMLU-Pro [ 145], GAOKAO [ 176],\nIFEval [184], SGP-Bench [102], VSI-Bench [160], ERQA [121], SpaCE-10 [38], and OmniSpatial [50].\nunderstanding [ 5,81,146,179,186], often comes with ever increasing computational costs, which have become\na crucial bottleneck of real-world applications.\nIn this work, we introduce InternVL3.5, an advanced family of InternVL series [ 13,14,15,37,79,80,186] with\nstronger capabilities in versatility, reasoning, and efficiency. Compared to InternVL3 [ 186], InternVL3.5 achieves\nsuperior performance through our proposed Cascade RL framework, which enhances reasoning capabilities in\nan efficient, scalable, and stable manner. Cascade RL consists of two complementary substages: an offline RL\nstage [ 141], which efficiently achieves satisfactory performance, and an online RL stage [ 182], which carefully\nrefines the output distribution and further push the performance upper bound of the model. The offline stage\nserves as an effective warm-up, ensuring high-quality rollouts for the subsequent online stage, thereby enabling\nthe progressive improvement of MLLM reasoning abilities. In practice, Cascade RL demonstrates promising\nsalability and stability, with a clear gain seen from InternVL3.5-1B to InternVL3.5-241B (Figure 5).\nIn addition, we equip InternVL3.5 with a much faster inference speed than its predecessor through two novel\nmethods, namely Visual Resolution Router (ViR) and Decoupled Vision-Language Deployment (DvD). In\nparticular, ViR aims to dynamically select the best trade-off resolution of visual tokens, thus reducing the\ninference costs with a negligible performance sacrifice. In practice, ViR can be efficiently integrated into\nInternVL3.5 with a light training stage namely Visual Consistency Learning (ViCO). Furthermore, DvD aims to\ndeploy ViTs and LLMs on separate GPUs to maximize computational parallelism and hardware utilization. These\ntwo methods can be seamlessly combined to realize the hardware-friendly implementation for InternVL3.5.\nWe conduct extensive experiments on public benchmarks to compare InternVL3.5 with existing MLLMs. As\nshown in Figure 1, the InternVL3.5 series consistently maintains a leading position among open source MLLMs\nin terms of overall score. Compared to the latest commercial model, i.e.,GPT-5 [ 98], InternVL3.5-241B-A28B\nnarrows the gap to 3.9%. In addition, our detailed ablation study demonstrates that InternVL3.5 achieves\nup to +16.0% improvement in overall reasoning performance and 4.05 \u00d7speed-up in inference efficiency\ncompared to its predecessor ( i.e., InternVL3 [ 186]). For example, InternVL3.5-8B and InternVL3.5-241B-A28B\nachieve scores of 73.4 and 77.7, respectively, on the MMMU benchmark [ 169], showing their strong reasoning\ncapabilities among existing open source MLLMs. In terms of versatility, InternVL3.5 remains competitive\nagainst both open-source and closed-source MLLMs in text tasks, GUI tasks, embodied tasks, SVG-based\nunderstanding and generation, etc. For example, InternVL3.5-30B-A3B and InternVL3.5-241B-A28B surpass\nthe latest open-source MLLM (Step-3 [129]) by +2.0 and +8.4 in text tasks, respectively.\n2\n\n--- Page 3 ---\nDynamic High Resolution Text TokenizerQwen3 / GPT-OSS\nInternViT-300M / InternViT-6B\nChat MessageVision-Language Connector\nSingle-Image\nVideo\nMulti-Image\n(b) Overall Model ArchitectureHigh ResolutionMLP ProjectorVisual Resolution RouterTop-1Low ResolutionInternVL3.5-Flash(c) Connector Architecture(a) Data Preprocessing  \n1:11:21:31:41:51:62:33:2...Pre-defined Aspect Ratios448\u00d7448 TilesMatching2:3 (896\u00d71344)ThumbnailInput Image (800\u00d71300)MLP ProjectorPixel ShuffleInternVL3.5Figure 2: Overall architecture. InternVL3.5 adopts the \u201cViT\u2013MLP\u2013LLM\u201d paradigm as in previous versions.\nBuilding upon InternVL3.5, we further introduce InternVL3.5-Flash, which is extended with an additional visual\nresolution router (ViR) to dynamically select the appropriate compression rate ( e.g.,1\n4or1\n16) for each image\npatch. Unlike Dynamic High Resolution which only splits image patches from the perspective of image width\nand height, our proposed ViR further introduces adaptivity from the perspective of semantic content.\nIn summary, our contributions include three folds:\n(1) We release InternVL3.5, the latest family of the InternVL series with advanced reasoning abilities, powerful\nversatility, and promising efficiency. InternVL3.5 comprises various model scales (from 1B to 241B) with both\ndense and mixture-of-experts (MoE) models. All of our models and codes are publicly released.\n(2) We propose three innovative designs in InternVL3.5, including cascade reinforcement learning (Cascade\nRL), visual resolution router (ViR) and decoupled vision-language deployment (DvD). These technologies\nsignificantly improve the capabilities and efficiency of InternVL3.5, providing practical tips to the community.\n(3) We conduct extensive experiments and demonstrate that InternVL3.5 exhibits leading performance among\nopen-source MLLMs [ 46,126,138,153,163]. Compared to the latest commercial model, i.e.,GPT-5 [ 98],\nInternVL3.5 even achieves slightly better results on general multimodal capabilities. We believe our approach\nand open source will further advance the community.\n2 InternVL3.5\nCompared to its predecessors, the InternVL3.5 series achieves superior performance and faster inference. In\nSection 2.1, we introduce the model architectures of InternVL3.5 and InternVL3.5-Flash. For InternVL3.5-Flash,\nwe further incorporate a Visual Resolution Router (ViR) module that dynamically selects the minimal resolution\nof visual tokens, achieving better inference efficiency. Section 2.2 and Section 2.3 describe the pre-training\nand post-training procedures of InternVL3.5, respectively. The details of our proposed Cascade Reinforcement\nLearning (Cascade RL) and Visual Consistency Learning (ViCO) methods are elaborated in Section 2.3. In\nSection 2.4, we present the test-time scaling approach used to further improve model performance. Finally, in\nSection 2.5, we describe the training and inference infrastructure supporting InternVL3.5, including implemen-\ntation details of the Decoupled Vision-Language Deployment (DvD) framework. The overall architecture is\nshown in Figure 2, and the training recipes are illustrated in Figure 3.\n2.1 Model Architecture\nInternVL3.5 . We follow the \u201cViT\u2013MLP\u2013LLM\u201d paradigm adopted in previous versions of InternVL [ 13,\n14,37,141,186]. As shown in Table 1, we initialize the language model using the Qwen3 series [ 158] and\nGPT-OSS [ 99], and the vision encoder using InternViT-300M and InternViT-6B [ 15]. The Dynamic High\nResolution strategy introduced in InternVL1.5 [14] is also retained in our design.\nInternVL3.5-Flash . Compared to InternVL3.5, InternVL3.5-Flash further integrates the Visual Resolution\nRouter (ViR), thus yielding a series of efficient variants suitable for resource-constrained scenarios. Specifically,\nin InternVL3.5, each image patch is initially represented as 1024 visual tokens for the vision encoder, which\nare then compressed into 256 tokens via a pixel shuffle module before being passed to the Large Language\nModel (LLM). In InternVL3.5-Flash, as shown in Figure 2, an additional pixel shuffle module with a higher\n3\n\n--- Page 4 ---\nModel Vision Encoder Language Model#Param\nVision Language Total\nDense Models\nInternVL3.5-1B InternViT-300M Qwen3-0.6B 0.3B 0.8B 1.1B\nInternVL3.5-2B InternViT-300M Qwen3-1.7B 0.3B 2.0B 2.3B\nInternVL3.5-4B InternViT-300M Qwen3-4B 0.3B 4.4B 4.7B\nInternVL3.5-8B InternViT-300M Qwen3-8B 0.3B 8.2B 8.5B\nInternVL3.5-14B InternViT-300M Qwen3-14B 0.3B 14.8B 15.1B\nInternVL3.5-38B InternViT-6B Qwen3-32B 5.5B 32.8B 38.4B\nMoE Models\nInternVL3.5-20B-A4B InternViT-300M GPT-OSS-20B 0.3B 20.9B 21.2B (A4B)\nInternVL3.5-30B-A3B InternViT-300M Qwen3-30B-A3B 0.3B 30.5B 30.8B (A3B)\nInternVL3.5-241B-A28B InternViT-6B Qwen3-235B-A22B 5.5B 235.1B 240.7B (A28B)\nTable 1: Pre-trained models used in the InternVL3.5 series . \u201cA\u201d denotes the number of activated parameters.\ncompression rate is included, enabling compression of visual tokens down to 64 tokens. For each patch, the\npatch router determines the appropriate compression rate by assessing its semantic richness, and routes it to the\ncorresponding pixel shuffle module accordingly. Benefiting from this patch-aware compression mechanism,\nInternVL3.5-Flash is able to reduce the number of visual tokens by 50% while maintaining nearly 100% of the\nperformance of InternVL3.5, as shown in Section 3.15.\n2.2 Pre-Training\nTraining Objective . During the pre-training stage, we update all model parameters jointly using the combination\nof large-scale text and multimodal corpora. Specifically, given an arbitrary training sample consisting of a\nmultimodal token sequence x= (x1, x2, . . . , x L), the next token prediction (NTP) loss [ 103] is calculated on\neach text token as follows:\nLi=\u2212logp\u03b8(xi|x1, . . . , x i\u22121), (1)\nwhere xiis the predicted token and prefix tokens in {x1, x2, . . . , x i\u22121}can be either text tokens or image tokens.\nIn particular, for conversation samples, only response tokens are included for the loss calculation. Additionally,\nto mitigate bias toward either longer or shorter responses during training, we adopt the square averaging [ 13] to\nreweight the NTP loss as follows:\nL\u2032\ni=wiP\njwj\u00b7 Li, w i=1\nN0.5, (2)\nwhere Ndenotes the number of tokens in the training sample on which the loss needs to be calculated. The\nrandom JPEG compression [13] is also included to enhance the model\u2019s real-world performance.\nData . The pre-training corpora can be classified into two categories: (1) Multimodal data: this subset of data is\nmainly sourced from the training corpora of InternVL3 [ 186], covering a diverse range of domains such as image\ncaptioning, general question answering, mathematics, scientific disciplines, charts, optical character recognition\n(OCR), knowledge grounding, document understanding, multi-turn dialogue, and medical data. (2) Text-only\ndata: this part of data is constructed based on the training corpora of InternLM series [ 9,124] and is further\naugmented with open-source datasets [ 6,73,75,94]. The pre-training corpora contains approximately 116M\nsamples, corresponding to about 250B tokens. The ratio between text-only and multimodal data is approximately\n1 : 2.5. The maximum sequence length is set to 32K tokens to adapt long-context understanding and reasoning.\n2.3 Post-Training\nAfter the pre-training stage, we adopt a three-stage post-training strategy comprising: (1) Supervised Fine-Tuning\n(SFT) , which maintains the same training objective as pre-training but leverages higher-quality conversation\ndata to further enhance the model\u2019s capabilities. (2) Cascade Reinforcement Learning (Cascade RL) , which\ncombines the benefits of offline and online RL methods to facilitate the reasoning capabilities. (3) Visual\nConsistency Learning (ViCO) , which aims to integrate visual resolution router (ViR) into InternVL3.5 to\nconstruct InternVL3.5-Flash, by minimizing the output divergence of different visual compression rates.\n4\n\n--- Page 5 ---\nNative \nPre-trainingSupervised\nFine-tuningCascade RL ViCO\n~250B tokens ~130B tokensMPO\nGSPOConsistency Training\nRouter Training\n270K samples ~30B tokens\nInternVL3.5\n InternVL3.5-FlashFigure 3: Training recipes of InternVL3.5. InternVL3.5 consists of three training stages: (1) native pre-training\nfor vision-language alignment, (2) supervised fine-tuning for adaptation to downstream tasks, (3) Cascade RL\nfor improvement on reasoning capabilities. InternVL3.5-Flash is an efficient version of InternVL3.5, which\nfurther integrates a visual resolution router (ViR) through consistency training and router training.\nSupervised Fine-Tuning . During the SFT phase, we adopt the same objective as in the pre-training stage and\nuse the square averaging strategy [ 13] to calculate the final loss. In this stage, the context window is set to\n32K tokens to adapt long-context information. Compared to InternVL3, the SFT stage of InternVL3.5 contains\nmore high-quality and diverse training data derived from three sources: (1) Instruction-following data from\nInternVL3, which are reused to preserve broad coverage of vision\u2013language tasks. (2) Multimodal reasoning\ndata in the \u201cThinking\u201d mode, which are included to instill long-thinking capabilities in the model. To construct\nsuch data, we leverage a large-scale reasoning model to sample rollouts with detailed reasoning processes. In\naddition to validating whether answers are factually correct, we implement strict filtering measures for the\nreasoning processes themselves: this includes evaluating how clear the thinking is, weeding out redundancy,\nand ensuring that formatting remains consistent. The questions in these datasets cover various expert domains,\nsuch as mathematics and scientific disciplines, thereby strengthening performance on different reasoning tasks.\n(3) Capability-expansion datasets, which endow InternVL3.5 with new skills, including GUI-based interaction,\nembodied interaction, and scalable vector graphics (SVG) understanding and generation.\nCascade Reinforcement Learning . Compared to Pre-training and Supervised Fine-tuning (SFT), the core\nadvantage of RL lies in its ability to introduce negative samples, which prune low-quality regions in the model\u2019s\noutput space and thereby enhance the overall response quality. As a derivative of the PPO algorithm [ 108],\nDPO [ 104] enables training based on existing rollouts, which we also regard as a form of offline RL. Offline RL\nalgorithms [ 104,141] often offer a higher training efficiency, but their performance ceiling is generally lower\ncompared to online RL methods. In contrast, despite the effectiveness of online RL algorithms [ 108,109,166,\n183], they are often computationally expensive and time-consuming. In this work, we propose Cascade RL,\nwhich aims to combine the benefits of offline RL and online RL to progressively facilitate the post-training of\nMLLMs in an efficient manner. Specifically, we first fine-tune the model using an offline RL algorithm [ 141]\nas an efficient warm-up stage to reach satisfied results, which can guarantee high-quality rollouts for the latter\nstage. Subsequently, we employ an online RL algorithm [ 182] to further refine the output distribution based\non rollouts generated by the model itself. Compared to the single offline or online RL stage, our cascaded RL\nachieves significant performance improvements at a fraction of the GPU time cost.\nDuring the offline RL stage, we employ mixed preference optimization (MPO) [ 141] to fine-tune the model.\nSpecifically, the training objective of MPO is a combination of preference loss Lp, quality loss Lq, and generation\nlossLg, which can be formulated as follows:\nLMPO=wpLp+wqLq+wgLg, (3)\nwhere w\u2217represents the weight assigned to each loss component. The DPO loss [ 104], BCO loss [ 53], and LM\nloss [8] serve as the preference loss, quality loss, and generation loss, respectively.\nDuring the online RL stage, we employ GSPO [ 182], without reference model constraints, as our online RL\nalgorithm, which we find more effective in training both dense and mixture-of-experts (MoE) models. Similar to\nGRPO [ 109], the advantage is defined as the normalized reward across responses sampled from the same query:\nbAi=r(x, yi)\u2212mean\u0010\n{r(x, yi)}G\ni=1\u0011\nstd\u0010\n{r(x, yi)}G\ni=1\u0011 , (4)\n5\n\n--- Page 6 ---\nwhere yiis the i-th response generated for the query x,Gis the total number of generated responses to the query,\nandr(x, yi)denotes the reward for this response. The training objective of GSPO is given by:\nLGSPO (\u03b8) =Ex\u223cD,{yi}G\ni=1\u223c\u03c0\u03b8old(\u00b7|x)\"\n1\nGGX\ni=1min\u0010\nsi(\u03b8)bAi,clip (si(\u03b8),1\u2212\u03b5,1 +\u03b5)bAi\u0011#\n, (5)\nwhere the importance sampling ratio is defined as the geometric mean of the per-token ratios:\nsi(\u03b8) =\u0012\u03c0\u03b8(yi|x)\n\u03c0\u03b8old(yi|x)\u00131\n|yi|\n= exp\uf8eb\n\uf8ed1\n|yi||yi|X\nt=1log\u03c0\u03b8(yi,t|x, yi,<t)\n\u03c0\u03b8old(yi,t|x, yi,<t)\uf8f6\n\uf8f8, (6)\nwhere \u03c0\u03b8(yi|x, yi,<t)and\u03c0\u03b8(yi,t|x, yi,<t)denote the generation probability of response yiand token yi,t\nunder the policy model with parameters \u03b8, respectively.\nCompared to directly training the model with a single RL paradigm, Cascade RL offers the following advantages:\n(1)Better training stability : In the offline RL stage, the rollout collection and parameter updates are decoupled,\neffectively mitigating issues such as reward hacking. During the online RL stage, we empirically observe that\nstronger models exhibit more stable and robust training dynamics. As a result, the performance gains achieved\nin the MPO stage further enhance the stability of the GSPO stage and reduce sensitivity to the algorithm.\n(2)Improved training efficiency : In the MPO stage, rollouts can be shared across different models, amortizing\nthe sampling cost typically incurred during online RL. (3) Higher performance ceiling : Moreover, as shown\nin Section 3.15, models fine-tuned with MPO take fewer training steps to achieve higher performance in the\nsubsequent online RL phase, further reducing training overhead.\nVisual Consistency Learning . We further include ViCO as an additional training stage to integrate the visual\nresolution router (ViR) into InternVL3.5, thereby reducing the inference cost of InternVL3.5. The obtained\nefficient version of InternVL3.5 are termed as InternVL3.5-Flash . In particular, ViCO comprises two stages:\n(1)Consistency training : In this stage, the entire model is trained to minimize the divergence between response\ndistributions conditioned on visual tokens with different compression rates. In practice, we introduce an extra\nreference model, which is frozen and initialized with InternVL3.5. Given a sample, each image patch is\nrepresented as either 256 or 64 tokens, and the training objective is defined as follows:\nLViCO=E\u03be\u223cR\"\n1\nNNX\ni=1KL\u0010\n\u03c0\u03b8ref(yi|y<i, I)\n\n\n\u03c0\u03b8policy (yi|y<i, I\u03be)\u0011#\n, (7)\nwhere KLdenotes the KL divergence and \u03bedenotes the compression rate, which is uniformly sampled from\n{1\n4,1\n16}. The image I\u03beis represented as 256 tokens when \u03be=1\n4and 64 tokens when \u03be=1\n16. We note that the\nreference model always performs inference with \u03be=1\n4.\n(2)Router training : This stage aims to train the ViR to select an appropriate trade-off resolution for different\ninputs. ViR is formulated as a binary classifier and trained using standard cross-entropy loss. To construct the\nroute targets, we first compute the KL divergence between the model outputs conditioned on uncompressed\nvisual tokens ( i.e., 256 tokens per patch) and those conditioned on compressed visual tokens ( i.e., 64 tokens per\npatch). During this stage, the main MLLM (ViT, MLP and LLM) is kept frozen, and only the ViR is trained.\nSpecifically, we first compute the loss ratio for each patch:\nri=LViCO\u0000\nyi|I1\n16\u0001\nLViCO\u0000\nyi|I1\n4\u0001, (8)\nwhich quantifies the relative increase in loss caused by compressing the visual tokens. Based on this ratio, the\nbinary ground-truth label for the patch router is defined as:\nyrouter\ni=\u001a0, ri< \u03c4 (compression has negligible impact)\n1, ri\u2265\u03c4(compression has significant impact) ,(9)\nwhere yrouter\ni = 0andyrouter\ni = 1indicate that the compression rate \u03beis set to1\n16and1\n4, respectively. During\ntraining, we store the historical rivalues of a sliding window, and \u03c4is a dynamical threshold computed\nfrom the k- thpercentile of historical rivalues. In practice, the target distribution is balanced. During the\nconsistency training stage, all patches of the same image are represented with a random compression rate, in\norder to ensure that the model retains its capability when no compression is applied. As shown in Section 3.15,\nInternVL3.5-Flash reduces 50% of the visual tokens while maintaining nearly 100% of the original performance.\n6\n\n--- Page 7 ---\nData . For the supervised fine-tuning (SFT) stage, the datasets comprise approximately 56 million samples,\nwhich corresponds to around 130 billion tokens. The proportion of text-only data to multimodal data is roughly\n1:3.5. For the cascade reinforcement learning stage, we use MMPR-v1.2 [ 141] as the training data for offline\nRL, which contains about 200K sample pairs. Based on MMPR-v1.2, we compute the accuracy of each query\nusing the provided rollouts and select those whose model accuracy falls between 0.2 and 0.8 for online RL.\nWe further extend the dataset with recent multimodal datasets [ 70,83,90,135,162] to enhance diversity. The\nresulting dataset, termed MMPR-Tiny, consists of approximately 70K queries. We directly reuse the rollouts\nfrom MMPR-v1.2 for both offline RL and data filtering in online RL, thereby reducing the cost of sampling\nadditional rollouts.\nFor the ViCO stage, we primarily leverage datasets identical to the SFT stage during consistency training,\nensuring that the model retains its original performance. During router training, we use a subset of the SFT data,\nprimarily composed of OCR and VQA examples, which are rich in visual information and sometimes require\nhigh-resolution understanding. This enables the resolution router to learn how to dynamically decide whether\neach image patch can be compressed based on the visual information.\n2.4 Test-Time Scaling\nTest-time scaling (TTS) has been empirically demonstrated as an effective approach to enhance the reasoning\ncapabilities of LLMs and MLLMs, particularly for complex tasks that require multi-step inference [ 65,82,\n113,142,178]. In this work, we implement a comprehensive test-time scaling approach that simultaneously\nimproves reasoning depth ( i.e., deep thinking) and breadth ( i.e., parallel thinking). We note that unless otherwise\nspecified, the experimental results reported in Section 3 are obtained without applying TTS. Thus far, we have\nonly applied TTS to reasoning benchmarks, since we found that the model already exhibits strong perception\nand understanding capabilities, and initiating TTS yields no significant improvement.\nDeep Thinking . By activating the Thinking mode, we guide the model to deliberately engage in step-by-step\nreasoning ( i.e.,decomposing complex problems into logical steps and validating intermediate conclusions) prior\nto generating the final answer. This approach systematically improves the logical structure of solutions for\ncomplex problems, particularly those requiring multi-step inference, and enhances reasoning depth.\nParallel Thinking . Following InternVL3, for reasoning tasks, we adopt the Best-of-N (BoN) strategy by\nemploying VisualPRM-v1.1 [ 142] as the critic model to select the optimal response from multiple reasoning\ncandidates. This approach improves the reasoning breadth.\n2.5 Infrastructure\nTraining Framework . The model training is conducted mainly based on the XTuner framework [ 21], which\nincorporates a series of optimization strategies tailored for LLM and MoE training. These include fully shared\ndata parallelism (FSDP) [ 181] to partition model parameters across GPUs, data packing [ 13] to reduce padding\ntokens while balancing the token computation load across ranks for improved training efficiency, FP8 training\nbased on DeepGEMM [ 68] and liger-kernel\u2019s fused cross-entropy operator [ 47] to accelerate the training process,\nFlashAttention-3 [ 26,27] to support packed inputs and speed up attention computation, and the TMA-Adaptive\nFP8 Grouped GEMM kernel [ 118] to optimize the training of MoE models. For the online stage, we use\nverl [ 111] as our codebase. For InternVL3.5-20B-A4B, we implement an accelerated version of window\nattention with sink in GPT-OSS-20B through Triton [130].\nDecoupled Vision-Language Deployment . In multimodal inference, the vision encoder and language model\nhave distinct computational characteristics. The vision encoder that transforms images into semantic features is\nhighly parallelizable and does not rely on long-term history state. In contrast, the language model adopts the\ninference in an autoregressive manner, which requires previous states to compute the next one. This sequential\nproperty makes the language part more sensitive to memory bandwidth and latency. When MLLMs are deployed\nonline at scale, the vision and language models often block each other, thus incurring additional inference cost.\nThis effect becomes more pronounced with larger vision models or higher-resolution images.\nAs shown in Figure 4, we propose Decoupled Vision-Language Deployment (DvD) to address this issue by\nseparating vision and language processing, with a particular focus on optimizing the prefilling stage. The vision\nsubsystem batches and processes images to produce compact feature embeddings, which are then transmitted to\nthe language subsystem for fusion with the text context prior to decoding. This separation alleviates blocking and\nbrings multimodal prefilling performance closer to that of pure language models. In our system implementation,\nthe ViT and MLP (and ViR for InternVL3.5-Flash) are deployed on the vision server, while the language server\n7\n\n--- Page 8 ---\n(b) Decoupled Vision-Language DeploymentViT& MLPInput MessageLLMResponseInputMessageLLMViT& MLPResponseImageTextVision ServerLanguage ServerBF16(a) Vanilla DeploymentMixedComputational pipeline (time)VisionComputational pipeline (time)LanguageCommunicationShared Serverfor Vision and LanguageFigure 4: Overview of Decoupled Vision-Language Deployment. DvD decouples the vision and language\nmodels and deploys them on separate servers. The right side shows a time-consumption trace of the pipeline.\n(a)In the original deployment, the ViT, MLP, and LLM are executed sequentially. Given their substantial\ndifferences in size and computation patterns, this serial design significantly slows down inference. (b)With DvD,\nthe inference of the ViT and the LLM is performed in parallel and asynchronously. Thus, ViT\u2019s computations\ncan overlap with the LLM\u2019s prefilling and decoding, reducing resource conflicts and improving inference speed.\nexecutes only the LLM. The communication is unidirectional, transmitting BF16 visual features over TCP, with\nRDMA optionally employed to achieve a higher transmission speed. Vision processing, feature transmission,\nand language processing are organized into an asynchronous three-stage pipeline, enabling overlapped execution\nand minimizing pipeline stalls.\nDvD increases GPU utilization and processing efficiency on the vision side, while enabling the language server\nto focus exclusively on the LLM\u2019s prefilling and decoding without being blocked by vision computation. This\ndesign leads to improved throughput and responsiveness. Moreover, the architecture supports independent\nhardware cost optimization for the vision and language modules, and facilitates the seamless integration of new\nmodules without requiring modifications to the language server deployment.\n3 Experiments\nHere, we first compare the overall performance of InternVL3.5 with recent leading multimodal large language\nmodels (MLLMs) in Section 3.1. Subsequently, we evaluate our models in various domains, including mul-\ntimodal reasoning and mathematics (Section 3.2), optical character recognition (OCR), chart and document\nunderstanding (Section 3.3), multi-image understanding (Section 3.4), real-world comprehension (Section 3.5),\ncomprehensive multimodal evaluation (Section 3.6), multimodal hallucination evaluation (Section 3.7), visual\ngrounding (Section 3.8), multimodal multilingual understanding (Section 3.9), video understanding (Sec-\ntion 3.10), GUI (Section 3.11), embodied (Section 3.12), and SVG (Section 3.13) tasks, most of which were\ntested using VLMEvalKit [ 31]. Additionally, the evaluation of the language capabilities of InternVL3.5 is\npresented in Section 3.14. Finally, we ablate newly proposed designs in InternVL3.5, including the Cascade\nReinforcement Learning, Visual Resolution Router, and Decoupled Vision-Language Deployment (Section 3.15).\n3.1 Overall Comparison with Other Advanced MLLMs\nTable 2 presents a comprehensive evaluation of InternVL3.5\u2019s performance across 36 benchmarks catego-\nrized into four key multimodal task types: (1) General Tasks : MMStar [ 11], MMVet [ 167], MMBench V1.1\n(en) [ 71], MTVQA [ 119], AI2D [ 55], OCRBench [ 72], WildVision [ 78], MME-RealWorld (en) [ 177], Hal-\nlusionBench [ 41], MVBench [ 63], VideoMME [ 35], MLVU [ 185], LVBench [ 148]; (2) Reasoning Tasks :\nMMMU [ 169], MathVista [ 76], MathVision [ 76], MathVerse [ 174], DynaMath [ 188], WeMath [ 100], Olympiad-\nBench [ 43], LogicVista [ 152]; (3) Text-Centric Tasks : MATH500 [ 65], AIME24 [ 84], AIME25 [ 85],\n8\n\n--- Page 9 ---\nTask Benchmark InternVL3.5 InternVL3.5 GLM-4.1V Kimi-VL-2506 Qwen2.5-VL GLM-4.5V Step-3 GPT-5\nSize \u2013 30B-A3B 241B-A28B 9B 16B-A3B 72B 106B-A12B 321B-A38B \u2013\nGeneralMMStar 72.0 77.9 72.9 70.4 70.8 72.9 69.0* 75.7*\nMMVet 85.5 81.2 66.4* 78.1 76.2 75.2* 79.4* 77.6*\nMMBench V1.1 (en) 84.8 87.4 85.8 84.4 88.4 88.2 81.1* 88.6*\nMTVQA 33.7 39.3 25.5* 27.2* 31.7 30.5* 30.6* 33.1*\nAI2D (w/ mask) 86.8 87.3 87.9 81.9\u202088.7 88.1 83.7* 89.5*\nOCRBench 88.0 90.7 84.2 86.9 88.5 87.2 83.7* 80.7*\nWildVision 75.8 82.8 74.0* 64.8* 78.6* 79.0* 89.4* 77.4*\nMME-RealWorld (en) 64.8 65.1 61.7* 54.5* 63.2* 61.7* 54.0* 68.0*\nHallusionBench 53.8 57.3 63.2 59.8\u202055.2 65.4 64.2 65.2*\nMVBench 72.1 76.5 68.4 59.7\u202070.4 73.0 64.2* 74.0*\nVideoMME (w/o sub) 68.7 72.9 68.2 67.8 73.3 74.6 63.6* 81.8*\nMLVU 73.0 78.2 71.5* 74.2 74.6 75.3* 62.2* 77.3*\nLVBench 63.8 67.1 44.0 64.5 60.7 53.8 57.7* 72.6*\nOverall 71.0 74.1 67.2 67.2 70.8 71.1 67.9 74.0\nReasoningMMMU (val) 75.6 77.7 68.0 64.0 68.2\u202175.4 74.2 84.2\nMathVista 80.9 82.7 80.7 80.1 74.2\u202184.6 79.2* 81.9*\nMathVision 55.7 63.9 54.4 54.4\u202039.3\u202165.6 64.8 72.0*\nMathVerse (vision-only) 60.4 68.5 68.4 54.6\u202047.3\u202172.1 62.7* 81.2*\nDynaMath 36.5 46.5 42.5 28.1\u202035.9\u202153.9 50.1 60.9*\nWeMath 48.4 62.3 63.8 42.0\u202049.1\u202168.8 59.8* 71.1*\nOlympiadBench 62.9 68.7 56.3* 47.4\u202037.8* 64.0* 66.8* 73.2*\nLogicVista 55.7 66.7 60.4 51.4\u202055.7\u202162.4 60.2* 70.0*\nOverall 59.5 67.1 61.8 52.8 50.9 68.4 64.7 74.3\nTextMATH500 96.6 98.0 81.8* 91.8* 82.8* 94.2* 85.6* 97.8*\nAIME24 79.4 84.7 36.2* 54.0* 15.0* 80.1* 86.6* 90.0*\nAIME25 62.7 75.6 32.0* 39.1* 13.3* 72.8* 82.9 94.6\nGPQA 68.2 73.2 50.3* 42.3* 52.0* 56.6* 73.0 85.7\nMMLU-Pro 75.3 81.3 57.1* 68.5* 51.1* 69.7* 58.6* 85.6*\nC-Eval 83.2 90.9 72.3* 64.4* 88.2* 89.1* 84.7* 88.2*\nGAOKAO 91.9 94.5 78.4* 72.6* 92.9* 93.1* 70.2* 94.1*\nIFEval 74.3 83.7 71.5* 65.8* 83.9* 82.4* 73.4* 94.6*\nOverall 78.9 85.3 60.0 62.3 59.9 79.8 76.9 91.3\nAgenticSGP-Bench 69.4 70.7 57.1* 44.9* 57.1* 66.1* 56.5* 77.5*\nScreenSpot 86.6 89.8 \u2013 \u2013 87.1 \u2013 \u2013 \u2013\nScreenSpot-v2 87.3 92.9 \u2013 91.4 \u2013 \u2013 \u2013 \u2013\nOSWorld-G 42.4 53.2 \u2013 52.5 \u2013 \u2013 \u2013 \u2013\nVSI-Bench 63.7 69.5 39.2* 37.4* 36.1* 41.4* 34.2* 37.5*\nERQA 41.5 46.8 45.8\u202036.0\u202044.8\u202046.5\u202044.5\u202065.7*\nSpaCE-10 45.5 55.0 43.4* 39.2* 37.9* 51.6* 42.6* 43.8*\nOmniSpatial 48.1 51.9 47.7\u202037.3\u202047.9\u202051.0\u202047.0\u202059.6*\nOverall 60.6 66.2 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013\nTable 2: The overall comparison of InternVL3.5 series and existing open-source and closed-source MLLMs.\n*: reproduced through VLMEvalkit [31].\u2020: reported by GLM-4.5V [46].\u2021: reported by OpenCompass [20].\n9\n\n--- Page 10 ---\nGPQA [ 106], MMLU-Pro [ 61], GAOKAO [ 176], IFEval [ 184]; (4) Agentic Tasks : SGP-Bench [ 102],\nScreenSpot [16], ScreenSpot-v2 [149], OSWorld-G [155], VSI-Bench [160], ERQA [121], SpaCE-10 [38].\nWe report results of our flagship models (InternVL3.5-30B-A3B and InternVL3.5-241B-A28B) and frontier\nopen-source MLLMs (GLM-4.1V [ 46], Kimi-VL-A3B-2506 [ 125], GLM-4.5V [ 46], Qwen2.5-VL-72B [ 5] and\nStep-3 [129]). We also include a state-of-the-art closed-source MLLM (GPT-5 [98]) for comparison.\nThese results highlight InternVL3.5\u2019s strong capabilities across diverse tasks. For general multimodal tasks,\nInternVL3.5 demonstrates leading performance among open-source models on general multimodal understanding\n(e.g., MMVet), multilingual ( e.g., MTVQA), OCR ( e.g., OCRBench), real-world ( e.g., MME-RealWorld), and\nvideo ( e.g., LVBench) benchmarks. It even achieves a similar overall score as GPT-5 [ 98] (74.1 vs.74.0),\nthe state-of-the-art closed-source MLLM. Nevertheless, some tasks like HallusionBench still pose challenges,\nindicating the need for further refinements.\nWe also observe particularly significant gains in complex multimodal reasoning, as evidenced by the scores\nof 77.7 on MMMU and 82.7 on MathVista, which surpass most open-source models and approaching top-tier\ncommercial systems. These improvements are largely driven by enhanced training strategies, especially Cascade\nRL, and refined test-time scaling methodologies, enabling robust generalization in challenging domains such as\nmathematical reasoning (e.g., MathVerse) and multidisciplinary understanding (e.g., MTVQA).\nFor text-related tasks, InternVL3.5 outperforms most open-source models with significant margins. This is\nprimarily due to our native pre-training strategy introduced in InternVL3 [ 186], where we include a large amount\nof text-only data in the training process. This strategy allows the model to simultaneously acquire linguistic\nand multimodal abilities in a more efficient and integrated manner, and preserve the language capabilities of\nthe pre-trained LLM to avoid the catastrophic forgetting issue [ 80]. As a result, InternVL3.5 achieves high\nperformance on both general ( e.g., IFEval) and reasoning ( e.g., AIME24 and MMLU-Pro) benchmarks and\nnarrows the gap with GPT-5.\nWe also demonstrate the versatility of InternVL3.5 through agentic benchmarks. On SGP-Bench, an SVG\nunderstanding benchmark, InternVL3.5 achieves leading performance (69.4 and 70.7) that surpasses all open-\nsource models. For GUI tasks, our models show strong abilities on GUI grounding (ScreenSpot) and online\nagentic (OSWorld-G) benchmarks. Evaluation on embodied tasks (VSI-Bench, ERQA, SpaCE-10, OmniSpatial)\nvalidates InternVL3.5\u2019s spatial reasoning skills and the competence to understand complex and dynamic\nenvironments, highlighting its potential in embodied agents, robotic navigation, and interactive scene perception.\n3.2 Multimodal Reasoning and Mathematics\nTo comprehensively evaluate the multimodal reasoning and mathematical capabilities of InternVL3.5, we\nconduct extensive experiments across a series of benchmarks, including MMMU [ 169] for multidisciplinary\nreasoning, MathVista [ 76], MathVision [ 134], and MathVerse [ 174] for mathematical reasoning, as well as\nDynaMath [188], WeMath [100], and LogicVista [152] for complementary logical reasoning assessment.\nAs shown in Table 3, InternVL3.5 achieves state-of-the-art performance across all evaluated benchmarks among\nopen-source models. Compared with the previous generation InternVL3, InternVL3.5 improves reasoning\nperformance by more than 10 points across all model sizes relative to their counterparts of the comparable scale.\nFurthermore, InternVL3.5-241B-A28B consistently outperforms all open-source counterparts, obtaining an\noverall average score of 66.9. It is followed by InternVL3.5-38B, which secures the second highest score with\nan average of 66.0. At mid-scale, the performance of InternVL3.5 is particularly impressive, with InternVL3.5-\n30B-A3B and InternVL3.5-14B attaining scores of 75.6 and 73.3 on MMMU respectively, both outperforming\nthe larger InternVL3-78B (72.2). At lightweight scales, InternVL3.5 achieves substantial gains over open-source\nbaselines. Compared to its predecessor InternVL3, it delivers marked improvements: the InternVL3.5-2B\nmodel has an average score of 50.7, significantly higher than that of InternVL3-2B with a score of 32.4;\nthe InternVL3.5-4B model, with a score of 57.4, far exceeds the score of 33.5 of MiniCPM-V-4; and the\nInternVL3.5-8B model achieves a score of 60.3, substantially surpassing the score of 44.3 of InternVL3-8B.\nThese significant improvements are mainly from our Cascade RL, showing its strong scalability for reasoning\ntasks. The ablation study about how different training stages influence the reasoning abilities of our models is\npresented in Section 3.15.\nFurthermore, our experiments also demonstrate that Cascade RL can be seamlessly combined with parallel think-\ning and obtain further gains. For instance, with parallel thinking, the overall reasoning scores of InternVL3.5-4B,\nInternVL3.5-8B and InternVL3.5-241B-A28B are further improved by +2.6%, +2.1% and +1.8%, respectively,\nhighlighting the effectiveness of test-time scaling for reasoning-related tasks.\n10\n\n--- Page 11 ---\nModelMMMU\n(val)MathVista\n(mini)MathVisionMathVerse\n(vision-only)DynaMath\n(worst case)WeMath LogicVista Overall\nInternVL3-1B [186] 43.4 45.8 18.8 18.7 5.8 13.4 29.8 25.1\nInternVL3.5-1B 44.2 59.3 27.3 37.8 17.2 21.5 29.3 33.8\nw/ Parallel Thinking [142] 51.0 69.5 33.4 45.8 25.0 36.4 41.8 43.3\nOvis-2B [77] 45.6 64.1 17.7 29.4 10.0 9.9 34.7 30.2\nQwen2.5-VL-3B [5] 51.2 61.2 21.9 31.2 13.2 22.9 40.3 34.6\nInternVL3-2B [186] 48.6 57.0 21.7 25.3 14.6 22.4 36.9 32.4\nInternVL3.5-2B 59.0 71.8 42.8 53.4 31.5 48.5 47.7 50.7\nw/ Parallel Thinking [142] 64.8 74.3 49.0 54.2 33.3 52.6 49.4 53.9\nOvis-4B [77] 49.0 69.6 21.5 38.5 18.0 16.9 35.3 35.5\nMiniCPM-V-4-4B [163] 51.2 66.9 20.7 18.3 14.2 32.7 30.6 33.5\nInternVL2.5-4B [13] 51.8 64.1 18.4 27.7 15.2 21.2 34.2 33.2\nInternVL3.5-4B 66.6 77.1 54.4 61.7 35.7 50.1 56.4 57.4\nw/ Parallel Thinking [142] 71.4 79.2 57.5 60.0 38.3 53.0 60.4 60.0\nMiniCPM-o2.6 [163] 50.9 73.3 21.7 35.0 10.4 25.2 36.0 36.1\nOvis-8B [77] 57.4 71.8 25.9 42.3 20.4 27.2 39.4 40.6\nQwen2.5-VL-8B [5] 55.0 67.8 25.4 41.1 21.0 35.2 44.1 41.4\nMiMo-VL-RL-8B [153] 66.7 81.5 60.4 71.5 45.9 66.3 61.4 64.8\nKeye-VL-8B [126] 71.4 80.7 50.8 54.8 37.3 60.7 54.8 58.6\nGLM-4.1V-9B [46] 68.0 80.7 54.4 68.4 42.5 63.8 60.4 62.6\nInternVL3-8B [186] 62.7 71.6 29.3 39.8 25.5 37.1 44.1 44.3\nInternVL3.5-8B 73.4 78.4 56.8 61.5 37.7 57.0 57.3 60.3\nw/ Parallel Thinking [142] 73.4 80.8 59.9 62.6 39.9 57.6 62.6 62.4\nGemma-3-12B [122] 55.2 56.1 30.3 21.1 20.8 33.6 41.2 36.9\nOvis2-16B [77] 60.7 73.7 30.1 45.8 26.3 45.0 47.4 47.0\nInternVL3-14B [186] 67.1 75.1 37.2 44.4 31.3 43.0 51.2 49.9\nInternVL3.5-14B 73.3 80.5 59.9 62.8 38.7 58.7 60.2 62.0\nw/ Parallel Thinking [142] 74.3 81.5 61.4 64.1 41.3 60.8 61.3 63.5\nKimi-VL-A3B-2506 [125] 64.0 80.1 54.4 54.6 28.1 42.0 51.4 53.5\nInternVL3.5-20B-A4B 72.6 78.0 53.0 57.0 33.1 41.4 56.8 56.0\nw/ Parallel Thinking [142] 74.0 79.2 55.7 58.9 35.9 45.5 58.2 58.2\nInternVL3.5-30B-A3B 75.6 80.9 55.7 60.4 36.5 48.4 55.7 59.0\nw/ Parallel Thinking [142] 75.0 82.1 58.5 61.4 38.3 57.5 59.7 61.8\nGemma-3-27B [122] 64.9 59.8 39.8 34.0 28.5 37.9 47.3 44.6\nOvis2-34B [77] 66.7 76.1 31.9 50.1 27.5 51.9 49.9 50.6\nQwen2.5-VL-32B [5] 70.2 74.8 38.1 57.6 35.1 46.5 52.6 53.6\nSkywork-R1V3-38B [110] 76.0 77.1 52.6 59.6 35.1 56.5 59.7 59.5\nInternVL3-38B [186] 70.1 75.1 34.2 48.2 35.3 48.6 58.4 52.8\nInternVL3.5-38B 76.9 81.9 63.7 67.6 41.7 64.8 65.3 66.0\nw/ Parallel Thinking [142] 76.7 83.8 65.6 66.5 44.9 67.8 64.7 67.1\nGPT-5-nano-20250807 [98] 72.6 73.1 59.7 66.6 47.9 59.4 57.5 62.4\nGPT-5-20250807 [98] 81.8 81.9 72.0 81.2 60.9 71.1 70.0 74.1\nClaude-3.7-Sonnet [2] 75.0 66.8 41.9 46.7 39.7 49.3 58.2 53.9\nGemini-2.0-Pro [29] 69.9 71.3 48.1 67.3 43.3 56.5 53.2 58.5\nGemini-2.5-Pro [29] 74.7 80.9 69.1 76.9 56.3 78.0 73.8 72.8\nDoubao-1.5-Pro [42] 73.8 78.6 51.5 64.7 44.9 65.7 64.2 63.3\nGLM-4.5V [46] 75.4 84.6 65.6 72.1 53.9 68.8 62.4 69.0\nQvQ-72B-Preview [127] 70.3 70.3 34.9 48.2 30.7 39.0 58.2 50.2\nQwen2.5-VL-72B [5] 68.2 74.2 39.3 47.3 35.9 49.1 55.7 52.8\nStep3-321B-A38B [129] 74.2 79.2 64.8 62.7 50.1 59.8 60.2 64.4\nInternVL3-78B [186] 72.2 79.0 43.1 51.0 35.1 46.1 55.9 54.6\nInternVL3.5-241B-A28B 77.7 82.7 63.9 68.5 46.5 62.3 66.7 66.9\nw/ Parallel Thinking [142] 78.7 84.8 65.9 71.6 47.8 64.4 67.6 68.7\nTable 3: Comparison of multimodal reasoning and mathematical performance. MMMU [ 169] is a\nmultidisciplinary reasoning benchmark. MathVista [ 76], MathVision [ 134], MathVerse [ 174], DynaMath [ 188],\nand WeMath [ 100] are mathematics benchmarks. LogicVista [ 152] is a logical reasoning benchmark. Part of the\nresults are collected from other papers [ 46,126,138,153,186] and the OpenCompass leaderboard [ 20]. The\noverall score is the average score of all benchmarks.\n3.3 OCR, Chart, and Document Understanding\nTo evaluate the comprehensive capabilities of the model across tasks related to text, document, and chart compre-\nhension, we conduct an extensive assessment on nine benchmarks: AI2D [ 55], ChartQA [ 87], TextVQA [ 112],\nDocVQA [89], InfoVQA [88], OCRBench [72], SEED-2-Plus [59], CharXiv [147], and VCR [175].\nAs shown in Table 4, InternVL3.5 achieves competitive results on these benchmarks, outperforming other\nopen-source and closed-source models. At the lightweight scale, InternVL3.5 demonstrates significant potential.\nFor instance, InternVL3.5-2B attains an overall average score of 76.5 across nine benchmarks, surpassing\nInternVL3-2B of similar size, which scores 74.7. Specifically, on DocVQA, InfoVQA, and SEED-2-Plus,\nInternVL3.5-2B achieves scores of 89.4, 68.2, and 68.0, respectively, compared to 88.3, 66.1, and 64.6 for\nInternVL3-2B, highlighting InternVL3.5\u2019s strong vision-language understanding capabilities.\n11\n\n--- Page 12 ---\nModelAI2D\n(w / wo M)ChartQA\n(test avg)TextVQA\n(val)DocVQA\n(test)InfoVQA\n(test)OCR\nBenchSEED-2\nPlusCharXiv\n(RQ / DQ)VCR-EN-Easy\n(EM / Jaccard)Overall\nInternVL3-1B [186] 69.4 / 78.3 75.3 74.1 81.9 53.7 790 58.2 21.0 / 47.1 89.3 / 96.2 68.6\nInternVL3.5-1B 71.1 / 81.8 77.7 71.5 85.6 58.3 795 62.3 26.9 / 60.6 83.5 / 94.0 71.1\nQwen2-VL-2B [138] 74.7 / 84.6 73.5 79.7 90.1 65.5 809 62.4 \u2013 81.5 / \u2013 \u2013\nAquila-VL-2B [40] 75.0 / \u2013 76.5 76.4 85.0 58.3 772 63.0 \u2013 70.0 / \u2013 \u2013\nQwen2.5-VL-3B [5] 81.6 / \u2013 84.0 79.3 93.9 77.1 797 67.6 31.3 / 58.6 \u2013 \u2013\nInternVL3-2B [186] 78.7 / 87.4 80.2 77.0 88.3 66.1 835 64.6 28.3 / 54.7 91.2 / 96.9 74.7\nInternVL3.5-2B 78.8 / 89.1 80.7 76.5 89.4 68.2 836 68.0 31.6 / 65.0 90.1 / 96.4 76.5\nMiniCPM-V-4-4B [163] 80.9 / 91.4 73.0 81.4 94.0 67.0 862 67.0 31.9 / 56.4 80.9 / 90.1 75.0\nInternVL3.5-4B 82.6 / 92.3 86.0 77.9 92.4 75.0 815 69.4 39.6 / 71.1 91.6 / 97.0 79.7\nOvis1.6-Gemma2-9B [77] 84.4 / \u2013 \u2013 \u2013 \u2013 \u2013 830 \u2013 \u2013 \u2013 \u2013\nMiniCPM-V2.6-8B [163] 82.1 / \u2013 82.4 80.1 90.8 \u2013 852 65.7 31.0 / 57.1 73.9 / 85.7 \u2013\nMolmo-7B-D [30] \u2013 / 93.2 84.1 81.7 92.2 72.6 694 \u2013 \u2013 \u2013 \u2013\nQwen2-VL-7B [138] 83.0 / 92.1 83.0 84.3 94.5 76.5 866 69.0 \u2013 89.7 / 93.8 \u2013\nQwen2.5-VL-7B [5] 83.9 / \u2013 87.3 84.9 95.7 82.6 864 70.4 42.5 / 73.9 \u2013 \u2013\nKeye-VL-8B [126] 85.8 / 88.5 72.5 75.7 87.0 63.0 853 67.8 36.8 / 75.2 \u2013 \u2013\nGLM-4.1V-9B [46] 82.2 / 87.0 70.0 79.6 93.3 80.3 823 71.8 53.4 / 82.4 32.7 / 55.2 72.5\nInternVL3-8B [186] 85.2 / 92.6 86.6 80.2 92.7 76.8 880 69.7 37.6 / 73.6 94.5 / 98.1 81.3\nInternVL3-9B [186] 84.6 / 92.9 86.2 79.4 93.6 79.6 877 68.8 38.0 / 72.5 94.2 / 97.9 81.3\nInternVL3.5-8B 84.0 / 92.8 86.7 78.2 92.3 76.2 832 70.8 44.4 / 72.2 92.6 / 97.3 80.9\nGemma3-12B [122] 84.2 / 87.2 75.7 67.7 87.1 64.9 702 65.5 24.9 / 65.4 \u2013 \u2013\nInternVL3-14B [186] 86.0 / 93.7 87.3 80.5 94.1 83.6 875 70.3 43.1 / 82.2 94.8 / 98.2 83.4\nInternVL3.5-14B 85.1 / 93.3 86.5 77.8 93.4 78.3 836 70.7 47.9 / 76.6 93.4 / 97.7 82.0\nKimi-VL-A3B-2506 [125] 81.9 / 91.2 73.7 77.7 93.5 74.5 869 70.8 46.8 / 71.5 81.1 / 90.6 78.3\nInternVL3.5-20B-A4B 85.9 / 93.5 86.6 78.5 92.9 78.1 870 69.3 38.2 / 78.5 93.7 / 97.8 81.7\nInternVL3.5-30B-A3B 86.8 / 94.5 87.4 80.5 94.2 77.8 880 70.6 48.0 / 81.8 94.9 / 98.2 83.6\nGemma3-27B [122] 84.5 / 88.2 78.0 65.1 86.6 65.1 717 68.1 25.7 / 63.8 \u2013 \u2013\nQwen2.5-VL-32B [5] 84.0 / 92.9 64.9 78.9 94.8 83.4 850 72.1 46.1 / 84.9 88.9 / 93.0 80.7\nCambrian-34B [131] 79.5 / \u2013 75.6 76.7 75.5 46.0 600 \u2013 27.3 / 59.7 79.7 / 89.3 \u2013\nVILA-1.5-40B [66] 69.9 / \u2013 67.2 73.6 \u2013 \u2013 460 \u2013 24.0 / 38.7 \u2013 \u2013\nInternVL3-38B [186] 88.9 / 95.5 89.2 83.9 95.4 85.0 886 71.6 46.4 / 87.2 96.1 / 98.7 85.5\nInternVL3.5-38B 87.8 / 95.1 88.8 82.7 94.0 81.0 870 71.0 48.1 / 83.1 95.3 / 98.4 84.4\nGPT-4V [95] 78.2 / 89.4 78.5 78.0 88.4 75.1 645 53.8 37.1 / 79.9 52.0 / 65.4 70.0\nGPT-4o-20240513 [97] 84.6 / 94.2 85.7 77.4 92.8 79.2 736 72.0 47.1 / 84.5 91.6 / 96.4 81.6\nClaude-3-Opus [2] 70.6 / 88.1 80.8 67.5 89.3 55.6 694 44.2 30.2 / 71.6 62.0 / 77.7 67.3\nClaude-3.5-Sonnet [2] 81.2 / 94.7 90.8 74.1 95.2 74.3 788 71.7 60.2 / 84.3 63.9 / 74.7 78.7\nGemini-1.5-Pro [105] 79.1 / 94.4 87.2 78.8 93.1 81.0 754 \u2013 43.3 / 72.0 62.7 / 77.7 \u2013\nGLM-4.5V [46] 88.1 / 93.7 86.6 72.0 94.5 84.1 872 74.0 59.5 / 88.2 36.5 / 44.9 75.8\nNVLM-D-72B [25] 85.2 / 94.2 86.0 82.1 92.6 \u2013 853 \u2013 \u2013 \u2013 \u2013\nMolmo-72B [30] \u2013 / 96.3 87.3 83.1 93.5 81.9 \u2013 \u2013 \u2013 \u2013 \u2013\nQwen2-VL-72B [138] 88.1 / \u2013 88.3 85.5 96.5 84.5 877 \u2013 \u2013 91.3 / 94.6 \u2013\nQwen2.5-VL-72B [5] 88.7 / \u2013 89.5 83.5 96.4 87.3 885 73.0 49.7 / 87.4 \u2013 \u2013\nInternVL3-78B [186] 89.7 / 96.0 89.7 84.3 95.4 86.5 906 71.9 46.0 / 85.1 96.0 / 98.6 85.8\nInternVL3.5-241B-A28B 87.3 / 95.0 88.0 84.5 94.9 82.0 907 71.9 48.3 / 83.9 97.0 / 99.0 85.2\nTable 4: Comparison of OCR, chart, and document understanding performance. We compare OCR-related\nperformance across 9 benchmarks: AI2D [ 55], ChartQA [ 87], TextVQA [ 112], DocVQA [ 89], InfoVQA [ 88],\nOCRBench [ 72], SEED-2-Plus [ 59], CharXiv [ 147], and VCR [ 175]. Part of results are collected from the\nOpenCompass leaderboard [ 20] and other papers [ 2,30,32,147,175]. When calculating Overall, the score of\nOCR Bench is normalized from 0-1000 to 0-100.\nMoreover, as the model scale increases, InternVL3.5 continues to deliver enhanced performance in vision-\nlanguage understanding. Across the same nine benchmarks, InternVL3.5-4B, InternVL3.5-20B-A4B,\nInternVL3.5-14B, InternVL3.5-30B-A3B, and InternVL3.5-38B achieve overall average scores of 79.7, 81.7,\n82.0, 83.6, and 84.4, respectively, demonstrating consistent improvements with larger model sizes. On AI2D,\nInternVL3.5-2B, InternVL3.5-4B, InternVL3.5-14B and InternVL3.5-38B obtain scores of 78.8/89.1, 82.6/92.3,\n85.1/93.3, and 87.8/95.1, respectively, indicating a consistent upward trend across larger models. On ChartQA,\nInternVL3.5-4B shows a notable improvement in chart understanding, increasing from 80.7 (InternVL3.5-2B)\nto 86.0, reflecting a significant gain in visual reasoning capability. Similarly, on CharXiv and VCR-EN-Easy,\nlarger model variants also achieve measurable improvements in text and document understanding.\n3.4 Multi-Image Understanding\nTo assess InternVL3\u2019s ability to understand and reason over multiple images \u2014\u2013 a key aspect of multimodal\ninteraction \u2014\u2013 we conduct comprehensive evaluations on a suite of widely recognized benchmarks, including\nBLINK [ 36], Mantis-Eval [ 51], MMIU [ 91], MuirBench [ 132], MMT-Bench [ 165], and MIRB [ 180]. These\nbenchmarks evaluate critical skills such as cross-image reasoning and context integration, which are essential\nfor effective multimodal systems.\n12\n\n--- Page 13 ---\nModel NameBLINK\n(val)Mantis\nEvalMMIUMuir\nBenchMMT\n(val)MIRB\n(avg)OverallRealWorld\nQAMME-RW\n(EN)WildVision\n(win rate)R-Bench\n(dis)Overall\nInternVL3-1B [186] 42.9 50.2 39.3 31.2 52.9 36.1 42.1 58.2 46.0 43.8 60.4 52.1\nInternVL3.5-1B 44.0 54.8 45.2 41.7 54.5 44.2 47.4 57.6 46.8 49.2 57.4 50.6\nQwen2-VL-2B [138] 44.4 \u2013 \u2013 \u2013 55.1 \u2013 \u2013 62.6 \u2013 \u2013 \u2013 \u2013\nQwen2.5-VL-3B [5] 47.6 \u2013 \u2013 47.7 \u2013 \u2013 \u2013 65.4 53.1 \u2013 \u2013 \u2013\nInternVL3-2B [186] 50.3 65.9 43.0 38.8 59.5 42.9 50.1 64.3 53.8 48.8 67.5 58.6\nInternVL3.5-2B 51.3 58.5 44.9 44.0 58.5 45.9 50.5 62.0 49.7 66.6 62.4 58.5\nMiniCPM-V-4 [163] 53.0 71.4 \u2013 46.1 59.7 \u2013 \u2013 66.0 58.4 46.0 64.2 58.7\nInternVL3.5-4B 58.1 62.7 49.2 53.1 64.3 55.9 57.2 66.3 59.8 69.8 68.7 65.1\nQwen2-VL-7B [138] 53.2 \u2013 \u2013 \u2013 64.0 \u2013 \u2013 70.1 56.5 \u2013 64.0 \u2013\nQwen2.5-VL-7B [5] 56.4 \u2013 \u2013 59.6 \u2013 \u2013 \u2013 68.5 57.4 \u2013 \u2013 \u2013\nMiniCPM-V2.6 [163] 53.0 69.0 \u2013 \u2013 60.8 \u2013 \u2013 65.0 \u2013 \u2013 \u2013 \u2013\nKeye-VL-8B [126] 53.4 \u2013 \u2013 50.4 65.4 \u2013 \u2013 65.9 48.8 66.6 65.7 61.7\nGLM-4.1V-9B [46] 65.9 \u2013 \u2013 74.7 68.4 \u2013 \u2013 70.6 61.7 74.0 69.1 68.9\nInternVL3-8B [186] 55.5 70.1 46.8 55.0 65.0 56.8 58.2 70.8 62.0 69.8 74.1 69.2\nInternVL3-9B [186] 58.6 70.1 50.4 51.4 65.4 58.6 59.1 70.5 61.3 63.8 70.3 66.5\nInternVL3.5-8B 59.5 70.5 49.4 55.8 66.7 57.3 59.9 67.5 62.8 76.6 69.7 67.4\nGemma3-12B [122] 52.6 \u2013 \u2013 49.7 58.4 \u2013 \u2013 59.8 48.9 72.8 60.4 60.5\nInternVL3-14B [186] 60.3 76.0 50.9 56.2 70.3 59.3 62.2 70.7 64.0 69.8 69.3 68.5\nInternVL3.5-14B 57.6 73.7 51.3 58.0 68.0 59.0 61.3 70.5 63.2 73.0 70.9 69.4\nKimi-VL-A3B-2506 [125] 56.8 \u2013 \u2013 64.6 63.9 \u2013 \u2013 72.4 54.5 64.8 69.3 65.3\nInternVL3.5-20B-A4B 59.0 74.2 50.5 58.5 66.6 54.4 60.5 71.2 60.0 69.6 71.5 68.1\nInternVL3.5-30B-A3B 60.4 72.8 55.1 53.1 66.6 59.0 61.2 72.3 64.8 75.8 70.7 70.9\nGemma3-27B [122] 54.4 \u2013 \u2013 55.2 61.8 \u2013 \u2013 65.1 51.6 79.8 61.6 64.5\nQwen2.5-VL-32B [5] 61.2 \u2013 \u2013 64.5 66.4 \u2013 \u2013 71.2 60.3 85.2 69.5 71.6\nCambrian-34B [131] \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 67.8 44.1 \u2013 \u2013 \u2013\nInternVL3-38B [186] 64.0 77.9 57.4 63.8 71.8 62.3 66.2 75.6 67.3 71.6 73.3 72.0\nInternVL3.5-38B 60.9 77.4 58.9 63.7 71.8 71.9 67.4 75.9 66.0 80.0 73.1 73.8\nGPT-4V [95] 54.6 62.7 \u2013 62.3 64.3 53.1 \u2013 61.4 \u2013 71.8 65.6 \u2013\nGPT-4o-20240513 [97] 68.0 \u2013 55.7 68.0 65.4 \u2013 \u2013 75.4 45.2 80.6 77.7 69.7\nClaude-3.5-Sonnet [2] \u2013 \u2013 53.4 \u2013 \u2013 \u2013 \u2013 60.1 51.6 \u2013 \u2013 \u2013\nGemini-1.5-Pro [105] \u2013 \u2013 53.4 \u2013 64.5 \u2013 \u2013 67.5 38.2 \u2013 \u2013 \u2013\nGLM-4.5V [46] 65.3 \u2013 \u2013 75.3 70.9 \u2013 \u2013 75.2 61.7 79.0 72.9 72.2\nQwen2-VL-72B [138] \u2013 \u2013 \u2013 \u2013 71.8 \u2013 \u2013 77.8 \u2013 \u2013 \u2013 \u2013\nQwen2.5-VL-72B [5] 64.4 \u2013 \u2013 70.7 \u2013 \u2013 \u2013 75.7 63.2 \u2013 \u2013 \u2013\nInternVL3-78B [186] 66.3 79.3 60.4 64.5 73.2 64.3 68.0 78.0 65.4 73.6 77.4 73.6\nInternVL3.5-241B-A28B 61.4 77.0 61.3 47.5 72.7 73.0 65.5 75.2 65.1 82.8 75.4 74.6\nTable 5: Comparison of multi-image and real-world understanding performance. Multi-image benchmarks\ninclude BLINK [ 36], Mantis-Eval [ 51], MMIU [ 91], MuirBench [ 132], MMT-Bench [ 165], and MIRB [ 180].\nReal-world benchmarks include RealWorldQA [ 22], MME-RealWorld [ 177], WildVision [ 78], and R-Bench [ 60].\nPart of the results are sourced from the benchmark papers and the OpenCompass leaderboard [20].\nAs shown in Table 5, across various model scales, InternVL3.5 consistently outperforms other open-source\nand closed-source counterparts, including earlier versions such as InternVL3. For example, InternVL3.5-38B\nachieves an overall score of 67.4, which is higher than the 66.2 achieved by InternVL3-38B. On the lightweight\nscale, InternVL3.5-2B achieves an overall score of 50.5, and its performance on individual benchmarks is 51.3\non BLINK, 58.5 on Mantis, 44.9 on MMIU, 44.0 on Muir, 58.5 on MMT and 45.9 on MIRB. Furthermore,\nlarger model sizes lead to significant improvements in multi-image understanding capabilities. When the model\nis scaled up to InternVL3.5-4B, the overall score increases to 57.2, with scores of 58.1 on BLINK, 62.7 on\nMantis, 49.2 on MMIU, 53.1 on Muir, 64.3 on MMT, and 55.9 on MIRB. As the model size continues to grow,\nperformance across all benchmarks improves consistently, with InternVL3.5-8B achieving an overall score of\n59.9, InternVL3.5-14B reaching 61.3, and InternVL3.5-241B-A28B improving to 65.5.\n3.5 Real-World Comprehension\nTo evaluate the performance of InternVL3.5 on realistic and complex tasks, we provide experimental results on\nfour real-world comprehension benchmarks: RealWorldQA [ 22], MME-RealWorld [ 177], WildVision [ 78], and\nR-Bench [60].\nAs shown in Table 5, InternVL3.5 achieves comparable or superior performance compared to existing methods,\ne.g., Qwen2.5-VL, MiniCPM-V-4 and Keye-VL. For example, the smallest variant InternVL3.5-1B demonstrates\npromising performance with a RealWorldQA score of 57.6, an MME-RealWorld score of 46.8, a WildVision\nwin rate of 49.2, and an R-Bench score of 57.4. Scaling up the model results in further improvements, as larger\nmodels provide more robust representations and stronger comprehension capabilities in real-world scenarios.\nAt the higher end of the scale, the InternVL3.5-38B and InternVL3.5-241B-A28B models achieve top-tier\nresults among the InternVL3.5 series. In particular, InternVL3.5-241B-A28B records an overall score of 74.6.\nCompared to competitive models, such as GPT-4o [ 95]\u2014which scores 45.2 on MME-RealWorld and 80.6 on\n13\n\n--- Page 14 ---\nModelMME\n(sum)MMB v1.1\n(EN)MMVet\n(turbo)MMStar OverallHallBench\n(avg)CRPE\n(relation)POPE\n(avg)Overall\nInternVL3-1B [186] 1934.4 69.9 59.5 51.5 62.5 41.4 64.0 90.7 65.4\nInternVL3.5-1B 1910.2 69.9 56.5 51.9 61.6 41.0 68.4 86.8 65.4\nQwen2-VL-2B [138] 1872.0 72.2 49.5 48.0 59.1 41.7 \u2013 \u2013 \u2013\nQwen2.5-VL-3B [5] 2157.0 \u2013 \u2013 \u2013 \u2013 46.3 73.6 \u2013 \u2013\nInternVL3-2B [186] 2221.2 78.6 62.2 60.7 70.2 42.5 71.5 89.6 67.9\nInternVL3.5-2B 2123.3 76.6 71.7 62.7 71.7 48.6 75.6 87.2 70.5\nMiniCPM-V-4-4B [163] 2167.7 79.1 56.6 59.0 68.0 46.9 74.3 82.4 67.9\nInternVL3.5-4B 2272.3 80.3 76.6 65.0 75.8 44.8 75.0 88.9 69.6\nQwen2-VL-7B [138] 2326.8 80.7 62.0 60.7 71.6 50.6 74.4 88.1 71.0\nQwen2.5-VL-7B [5] 2347.0 82.6 67.1 63.9 74.4 52.9 76.4 86.4 71.9\nMiniCPM-V2.6 [163] 2348.4 78.0 60.0 57.5 69.8 48.1 75.2 87.3 70.2\nKeye-VL-8B [126] 2214.7 76.3 70.0 72.8 74.5 57.7 76.5 87.0 73.7\nGLM-4.1V-9B [46] 2445.8 85.8 66.4 72.9 78.1 63.2 78.9 87.1 76.4\nInternVL3-8B [186] 2415.4 81.7 81.3 68.2 79.4 49.9 76.3 91.1 72.4\nInternVL3-9B [186] 2372.8 81.7 76.2 66.3 77.2 51.2 75.0 90.4 72.2\nInternVL3.5-8B 2380.6 79.5 83.1 69.3 79.2 54.5 75.1 88.7 72.8\nGemma3-12B [122] 2044.7 71.8 64.9 56.1 66.5 47.2 69.1 85.2 67.2\nInternVL3-14B [186] 2478.3 83.5 80.2 68.8 80.3 55.1 77.3 90.2 74.2\nInternVL3.5-14B 2398.4 81.5 81.7 70.4 79.8 54.0 76.6 87.7 72.8\nKimi-VL-A3B-2506 [125] 2352.7 84.4 78.1 70.4 79.2 59.8 60.9 86.5 69.1\nInternVL3.5-20B-A4B 2318.1 85.2 80.3 70.4 79.7 52.0 76.8 89.4 72.7\nInternVL3.5-30B-A3B 2461.9 84.8 85.5 72.0 82.6 53.8 77.6 89.6 73.7\nGemma3-27B [122] 1816.9 77.2 68.3 61.7 68.0 47.9 71.64 85.8 68.4\nQwen2.5-VL-32B [5] 2402.9 85.2 69.6 67.8 77.1 53.7 77.0 86.8 72.5\nCambrian-34B [131] \u2013 78.3 53.2 54.2 \u2013 41.6 \u2013 \u2013 \u2013\nInternVL3-38B [186] 2523.6 86.9 83.9 71.5 83.1 57.1 77.1 90.6 74.9\nInternVL3.5-38B 2492.4 87.3 82.2 75.3 83.5 59.7 77.7 90.4 75.9\nGPT-4V [95] 1926.6 80.0 67.5 56.0 68.1 46.5 \u2013 \u2013 \u2013\nGPT-4o-20240513 [97] \u2013 83.1 69.1 64.7 \u2013 55.0 76.6 86.9 72.8\nClaude-3-Opus [2] 1586.8 60.1 51.7 45.7 53.5 37.8 \u2013 \u2013 \u2013\nClaude-3.5-Sonnet [2] \u2013 80.9 70.1 65.1 \u2013 55.5 \u2013 \u2013 \u2013\nGemini-1.5-Pro [105] \u2013 74.6 64.0 59.1 \u2013 45.6 \u2013 \u2013 \u2013\nGLM-4.5V [46] 2423.8 88.2 75.2 72.9 80.7 64.5 55.4 85.9 68.6\nQwen2-VL-72B [138] 2482.7 85.9 74.0 68.3 79.2 58.1 \u2013 \u2013 \u2013\nQwen2.5-VL-72B [5] 2448.0 88.4 76.2 70.8 80.7 55.2 79.2 \u2013 \u2013\nInternVL3-78B [186] 2549.8 87.7 81.3 72.5 83.1 59.1 79.2 90.3 76.2\nInternVL3.5-241B-A28B 2525.9 87.4 81.2 77.9 84.2 57.3 78.0 90.7 75.3\nTable 6: Comparison of comprehensive multimodal understanding and hallucination performance. Com-\nprehensive multimodal benchmarks include MME [ 34], MMBench [ 71], MMVet [ 167], and MMStar [ 11].\nHallucination-related benchmarks encompass HallusionBench [ 41], CRPE [ 143], and POPE [ 64]. Part of the\nresults are sourced from the benchmark papers and the OpenCompass leaderboard [ 20]. When calculating\nOverall, the score of MME is normalized from 0-2800 to 0-100.\nWildVision\u2014the InternVL3.5 series exhibits competitive strengths. InternVL3.5-241B-A28B not only surpasses\nGPT-4o on RealWorldQA and closely matches its R-Bench performance but also considerably outperforms it on\nMME-RealWorld, indicating a robust overall performance on tasks demanding both perceptual precision and\ncomprehensive understanding.\n3.6 Comprehensive Multimodal Understanding\nIn Table 6, we evaluate InternVL3.5 on a set of comprehensive multimodal understanding benchmarks, including\nMME [ 34], MMBench (English and Chinese) [ 71], MMBench v1.1 (English) [ 71], and MMVet [ 167] and\nMMStar [11].\nWe observe that InternVL3.5 outperforms existing methods like Keye-VL, Qwen2.5-VL and MiniCPM-V-4,\nespecially on MMStar and MMVet. For instance, InternVL3.5-4B achieves an MMVet score of 76.6 and MMStar\nof 65.0, compared to 56.6 and 59.0 of MiniCPM-V-4. The improvements remain significant as model size grows,\nwhere InternVL3.5-241B-A28B finally achieves 87.4 on MMBench v1.1, 81.2 on MMVet, 77.9 on MMStar, and\nan overall score of 84.2.\n14\n\n--- Page 15 ---\nModel NameRefCOCO RefCOCO+ RefCOCOgOverallval test-A test-B val test-A test-B val test\nGrounding-DINO-L [69] 90.6 93.2 88.2 82.8 89.0 75.9 86.1 87.0 86.6\nUNINEXT-H [157] 92.6 94.3 91.5 85.2 89.6 79.8 88.7 89.4 88.9\nONE-PEACE [139] 92.6 94.2 89.3 88.8 92.2 83.2 89.2 89.3 89.8\nInternVL3-1B [186] 85.8 90.1 81.7 76.6 84.1 69.2 82.8 82.6 81.6\nInternVL3.5-1B 85.4 89.7 80.2 77.7 85.5 69.5 81.9 81.6 81.4\nInternVL3-2B [186] 89.8 92.6 86.4 84.0 89.2 76.5 87.6 87.2 86.7\nInternVL3.5-2B 88.7 91.6 84.8 82.7 88.4 76.6 85.6 85.5 85.5\nQwen2.5-VL-3B [5] 89.1 91.7 84.0 82.4 88.0 74.1 85.2 85.7 85.0\nInternVL3.5-4B 92.5 94.3 88.2 87.6 92.3 81.6 89.6 89.3 89.4\nShikra-7B [10] 87.0 90.6 80.2 81.6 87.4 72.1 82.3 82.2 82.9\nCogVLM-Grounding [140] 92.8 94.8 89.0 88.7 92.9 83.4 89.8 90.8 90.3\nQwen2-VL-7B [138] 91.7 93.6 87.3 85.8 90.5 79.5 87.3 87.8 87.9\nQwen2.5-VL-7B [5] 90.0 92.5 85.4 84.2 89.1 76.9 87.2 87.2 86.6\nTextHawk2 [168] 91.9 93.0 87.6 86.2 90.0 80.4 88.2 88.1 88.2\nInternVL3-8B [186] 92.5 94.6 88.0 88.2 92.5 81.8 89.6 90.0 89.6\nInternVL3-9B [186] 91.8 93.2 86.6 86.4 91.0 79.9 88.0 88.5 88.2\nInternVL3.5-8B 92.4 94.7 88.7 87.9 92.4 82.4 89.6 89.4 89.7\nFerret-v2-13B [172] 92.6 95.0 88.9 87.4 92.1 81.4 89.4 90.0 89.6\nInternVL3-14B [186] 92.0 94.4 87.8 87.4 92.1 81.5 88.6 89.3 89.1\nInternVL3.5-14B 92.6 94.7 89.4 88.3 92.7 82.5 90.1 90.5 90.1\nInternVL3.5-20B-A4B 91.9 94.1 88.8 87.6 92.0 82.7 89.1 90.0 89.5\nInternVL3.5-30B-A3B 93.1 95.4 90.1 89.6 93.2 84.4 90.6 91.0 90.9\nInternVL3-38B [186] 93.2 95.1 90.2 89.8 93.2 85.2 91.4 91.5 91.2\nInternVL3.5-38B 90.3 91.8 89.0 87.5 90.0 84.7 89.7 89.9 89.1\nQwen2-VL-72B [138] 93.2 95.3 90.7 90.1 93.8 85.6 89.9 90.4 91.1\nQwen2.5-VL-72B [5] 92.7 94.6 89.7 88.9 92.2 83.7 89.9 90.3 90.3\nInternVL3-78B [186] 93.4 95.4 90.3 90.1 93.8 85.3 91.5 91.5 91.4\nInternVL3.5-241B-A28B 94.1 96.3 91.5 91.6 94.6 86.9 92.0 92.1 92.4\nTable 7: Comparison of visual grounding performance. We evaluate InternVL3.5\u2019s visual grounding capability\non RefCOCO, RefCOCO+, and RefCOCOg datasets [54, 86]. Part of the results are collected from [138].\nWe note that InternVL3.5 does not achieve a notable improvement compared to InternVL3. This is partly\nbecause the model\u2019s understanding performance has approached saturation, and also partly stems from our\noptimization of text and reasoning capabilities\u2014which, while achieving improvements on relevant benchmarks,\nslightly impairs the performance of multimodal understanding.\n3.7 Multimodal Hallucination Evaluation\nTo evaluate the propensity for hallucination of InternVL3.5, we conduct experiments on three established\nbenchmarks: HallusionBench [ 41], CRPE [ 143], and POPE [ 64]. The results are shown in Table 6. Compared\nwith previous InternVL series, the new InternVL3.5 models provide consistent improvements in handling\nmultimodal hallucination challenges across various model scales, e.g., +2.6 on 2B scale and +1.0 on 38B scale\non the overall score. Despite these advancements, there are minor declines on some model scales such as 14B\nand 241B-A28B, indicating that further enhancement on data and training strategies is needed to achieve more\nconsistent improvements on all model scales, which remains an important future direction to build a more\ntrustworthy multimodal model.\n3.8 Visual Grounding\nFor the visual grounding task, we evaluate InternVL3.5 on RefCOCO, RefCOCO+, and RefCOCOg datasets [ 54,\n86]. As shown in Table 7, InternVL3.5 maintains the strong capabilities of InternVL3, which already achieves the\nupper bound of this tasks, i.e.,approximately 90% average accuracy. However, the InternVL3.5 training scheme\nstill provides additional gains on several model sizes. For example, InternVL3.5-14B achieves an overall score\nof 90.1 on the RefCOCO series, outperforming InternVL3-14B by +0.8%. In addition, InternVL3.5-241B-A28B\nbuilds a new state-of-the-art performance on RefCOCO, with an overall score of 92.4, further highlighting the\npotential of InternVL3.5 for real-world applications requiring precise multimodal understanding.\n15\n\n--- Page 16 ---\nModelMMMB Multilingual MMBench MTVQAOverallen zh pt ar tr ru en zh pt ar tr ru (avg)\nInternVL3-1B [186] 79.4 70.1 62.3 58.0 47.6 61.9 72.6 66.2 62.3 48.0 39.5 60.3 22.2 47.9\nInternVL3.5-1B 77.0 73.1 67.2 59.0 53.5 66.3 71.2 66.3 61.7 45.8 45.7 60.2 22.9 49.1\nQwen2-VL-2B [138] 78.3 74.2 72.6 68.3 61.8 72.8 72.1 71.1 69.9 61.1 54.4 69.3 20.0 52.6\nQwen2.5-VL-3B [5] \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 24.8 \u2013\nInternVL3-2B [186] 81.9 78.3 75.4 68.6 62.9 74.6 81.3 77.8 75.9 66.4 59.5 70.7 26.7 57.4\nInternVL3.5-2B 80.2 77.7 75.9 68.5 69.1 76.3 78.4 75.9 73.7 63.7 62.0 71.4 28.5 58.0\nMiniCPM-V-4-4B [163] 82.0 80.2 75.6 60.1 63.8 71.7 80.8 80.4 71.6 51.9 59.1 67.7 22.6 54.5\nInternVL3.5-4B 84.3 82.6 81.0 76.4 75.2 81.4 81.5 81.1 76.7 71.0 72.4 75.7 29.6 62.1\nmPLUG-Owl2 [164] 67.3 61.0 59.7 45.8 45.4 62.6 66.2 59.4 58.2 37.9 47.7 60.4 \u2013 \u2013\nQwen2-VL-7B [138] 83.9 82.4 81.2 79.0 74.7 82.4 81.8 81.6 79.1 75.6 74.5 79.3 25.6 61.6\nQwen2.5-VL-7B [5] \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 29.2 \u2013\nKeye-VL-8B [126] 66.8 83.0 74.1 73.8 72.0 76.8 53.9 87.8 57.4 67.2 67.14 68.7 22.3 54.6\nGLM-4.1V-9B [46] 82.6 83.6 79.4 80.4 80.4 82.9 83.0 86.0 79.8 78.8 78.5 82.0 25.5 62.8\nInternVL3-8B [186] 85.1 83.1 82.5 81.6 76.2 83.4 85.5 85.6 83.2 79.2 75.9 82.6 30.2 64.7\nInternVL3-9B [186] 84.8 83.7 80.6 69.9 68.5 80.8 86.5 85.2 79.1 64.3 68.3 79.1 27.1 60.7\nInternVL3.5-8B 84.9 83.0 81.4 79.6 77.4 82.1 82.5 80.7 79.0 75.9 74.8 77.6 35.2 65.0\nGemma3-12B [122] 77.6 77.3 77.3 73.4 73.9 75.9 72.1 72.5 68.3 53.8 60.5 60.4 24.4 55.0\nInternVL3-14B [186] 85.7 84.7 83.1 83.7 79.3 83.6 86.7 85.8 83.2 81.1 80.7 83.8 31.6 66.2\nInternVL3.5-14B 85.1 84.1 82.7 80.3 79.4 83.5 84.0 83.7 80.0 77.8 77.0 77.0 34.2 65.5\nKimi-VL-A3B-2506 [125] 83.1 78.9 76.9 71.3 71.0 76.3 82.3 79.9 76.9 62.8 66.7 73.8 27.2 59.1\nInternVL3.5-20B-A4B 85.1 83.1 83.2 82.2 80.4 83.8 85.3 84.2 82.1 78.9 79.5 82.5 28.2 64.4\nInternVL3.5-30B-A3B 86.4 85.7 83.4 83.3 81.7 85.0 86.1 86.3 83.1 82.3 81.5 83.5 33.7 67.3\nGemma3-27B [122] 78.9 77.8 77.8 75.4 76.1 76.9 79.0 77.2 74.5 71.8 74.0 74.1 27.5 59.9\nQwen2.5-VL-32B [5] 85.2 83.0 81.9 81.8 79.9 83.5 88.1 85.9 80.2 81.6 79.7 84.5 31.4 65.7\nInternVL3-38B [186] 86.7 85.6 84.5 84.8 82.6 85.1 89.0 89.3 87.1 84.6 84.3 87.4 32.4 68.1\nInternVL3.5-38B 86.7 85.5 85.1 84.1 84.3 85.3 87.4 86.9 84.2 82.0 83.4 85.6 36.1 68.7\nGPT-4V [95] 75.0 74.2 71.5 73.5 69.0 73.1 77.6 74.4 72.5 72.3 70.5 74.8 22.0 56.1\nGPT-4o [97] \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 27.8 \u2013\nGemini-1.0-Pro [120] 75.0 71.9 70.6 69.9 69.6 72.7 73.6 72.1 70.3 61.1 69.8 70.5 \u2013 \u2013\nGLM-4.5V [46] 87.1 86.9 84.8 84.5 84.6 84.3 89.1 89.3 86.9 83.7 84.0 87.2 30.5 67.5\nQwen2-VL-72B [138] 86.8 85.3 85.2 84.8 84.2 85.3 86.9 87.2 85.8 83.5 84.4 85.3 30.9 67.2\nQwen2.5-VL-72B [5] \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 31.7 \u2013\nInternVL3-78B [186] 87.2 86.6 85.5 86.5 84.6 86.1 89.4 90.3 88.7 86.1 86.6 88.1 32.5 68.9\nInternVL3.5-241B-A28B 87.6 86.4 85.3 84.2 85.1 86.0 88.9 87.7 87.0 86.5 86.7 87.6 39.3 70.8\nTable 8: Comparison of multimodal multilingual performance. We evaluate multilingual capabilities across\n3 benchmarks, including MMMB [ 114], Multilingual MMBench [ 114] and MTVQA [ 119] with six languages:\nEnglish (en), Chinese (zh), Portuguese (pt), Arabic (ar), Turkish (tr), and Russian (ru).\n3.9 Multimodal Multilingual Understanding\nInternVL3.5 exhibits strong multimodal multilingual understanding across a variety of benchmarks and lan-\nguages. As summarized in Table 8, InternVL3.5 consistently achieves high scores on MMMB [ 114], Multilingual\nMMBench [ 114] and MTVQA [ 119], covering six languages including English, Chinese, Portuguese, Arabic,\nTurkish, and Russian. Compared to InternVL3, InternVL3.5 has significant improvements in its language capa-\nbilities, thus achieving better results on these multilingual benchmarks. For example, InternVL3.5-1B achieves\nup to 1.2% gains over InternVL3-1B. Compared to other leading multimodal models, such as Qwen2.5-VL and\nGPT-4V , InternVL3.5 also demonstrates notable improvements in both overall accuracy and language coverage,\nespecially at larger scales. For example, InternVL3.5-241B-A28B outperforms GPT-4V by +14.7% on the\noverall score across all multilingual benchmarks. The results highlight InternVL3.5\u2019s robust capability to handle\ncomplex multilingual and multimodal tasks, making it highly effective for global applications that require\ncomprehensive cross-language understanding.\n16\n\n--- Page 17 ---\nModel NameVideo-MME\n(wo / w sub)MVBenchMMBench-Video\n(val)MLVU\n(M-Avg)LongVideoBench\n(val total)Overall\nInternVL3-1B [186] 51.0 / 53.0 63.1 1.30 53.0 48.1 51.9\nInternVL3.5-1B 52.4 / 55.0 61.0 1.39 56.6 53.0 54.1\nQwen2-VL-2B [138] 55.6 / 60.4 63.2 \u2013 \u2013 \u2013 \u2013\nQwen2.5-VL-3B [5] 61.5 / 67.6 67.0 1.63 68.2 43.3 60.3\nInternVL3-2B [186] 58.9 / 61.4 70.4 1.42 64.2 55.4 59.6\nInternVL3.5-2B 58.4 / 61.9 65.9 1.56 64.4 57.4 60.0\nMiniCPM-V-4-4B [163] 61.2 / 65.8 58.7 \u2013 \u2013 \u2013 \u2013\nInternVL3.5-4B 65.4 / 68.6 71.2 1.59 70.4 60.8 64.9\nVideoChat2-HD [62] 45.3 / 55.7 62.3 1.22 47.9 \u2013 \u2013\nLLaV A-OneVision-7B [58] 58.2 / \u2013 56.7 \u2013 \u2013 \u2013 \u2013\nMiniCPM-V-2.6 [163] 60.9 / 63.6 \u2013 1.70 \u2013 54.9 \u2013\nQwen2-VL-7B [138] 63.3 / 69.0 67.0 1.44 \u2013 55.6 \u2013\nQwen2.5-VL-7B [5] 65.1 / 71.6 69.6 1.79 70.2 45.3 63.6\nKeye-VL-8B [126] 67.7 / \u2013 \u2013 \u2013 \u2013 64.8 \u2013\nGLM-4.1V-9B [126] 68.2 / 73.6 68.4 1.63 71.5 44.0 63.3\nInternVL3-8B [186] 66.3 / 68.9 75.4 1.69 71.4 58.8 66.2\nInternVL3-9B [186] 66.7 / 68.9 74.3 1.69 70.8 62.5 66.6\nInternVL3.5-8B 66.0 / 68.6 72.1 1.67 70.2 62.1 65.8\nInternVL3-14B [186] 70.4 / 73.0 76.6 1.73 73.3 63.9 69.1\nInternVL3.5-14B 67.9 / 71.0 72.8 1.73 72.1 62.7 67.4\nKimi-VL-A3B-2506 [125] 67.8 / 72.6 59.7 \u2013 74.2 64.5 \u2013\nInternVL3.5-20B-A4B 62.4 / 64.9 73.3 1.54 65.6 58.3 62.6\nInternVL3.5-30B-A3B 68.7 / 71.8 72.1 1.69 73.0 63.8 67.6\nOryx-1.5-32B [74] 67.3 / 74.9 70.1 1.52 72.3 \u2013 \u2013\nQwen2.5-VL-32B [5] 70.5 / 77.9 \u2013 1.93 \u2013 \u2013 \u2013\nVILA-1.5-40B [66] 60.1 / 61.1 \u2013 1.61 56.7 \u2013 \u2013\nInternVL3-38B [186] 72.7 / 75.0 76.9 1.81 77.8 67.3 71.7\nInternVL3.5-38B 70.9 / 74.2 75.0 1.90 77.0 65.7 71.0\nGPT-4V/4T [1] 59.9 / 63.3 43.7 1.53 49.2 59.1 54.4\nGPT-4o-20240513 [95] 71.9 / 77.2 \u2013 1.63 64.6 66.7 \u2013\nGPT-4o-20240806 [97] \u2013 \u2013 1.87 \u2013 \u2013 \u2013\nGemini-1.5-Pro [105] 75.0 / 81.3 \u2013 1.30 \u2013 64.0 \u2013\nGLM-4.5V [58] 74.6 / 80.7 73.0 2.05 75.3 53.8 71.0\nVideoLLaMA2-72B [17] 61.4 / 63.1 62.0 \u2013 \u2013 \u2013 \u2013\nLLaV A-OneVision-72B [58] 66.2 / 69.5 59.4 \u2013 66.4 61.3 \u2013\nQwen2-VL-72B [138] 71.2 / 77.8 73.6 1.70 \u2013 \u2013 \u2013\nQwen2.5-VL-72B [5] 73.3 / 79.1 70.4 2.02 74.6 60.7 70.9\nInternVL3-78B [186] 72.7 / 75.7 78.7 1.81 79.5 65.7 72.1\nInternVL3.5-241B-A28B 72.9 / 76.0 76.5 1.74 78.2 67.1 71.4\nTable 9: Comparison of video understanding performance. We evaluate InternVL3.5\u2019s video understand-\ning capabilities across 5 benchmarks. For Video-MME [ 35], MMBench-Video [ 33], MLVU [ 185], and\nLongVideoBench [ 148], we test with four different settings: 16, 32, 48, and 64 frames, and report the maximum\nresults. For MVBench [ 63], we conduct testing using 16 frames. When calculating Overall, the score of\nMMBench-Video is normalized from 0-3 to 0-100.\n3.10 Video Understanding\nInternVL3.5 demonstrates remarkable video understanding capabilities across a comprehensive set of bench-\nmarks. As presented in Table 9, InternVL3.5 consistently achieves competitive or leading scores on Video-\nMME [ 35], MVBench [ 63], MMBench-Video [ 33], MLVU [ 185], LongVideoBench [ 148]. Performance\nimprovements are observed across almost all metrics for small-size models. In particular, InternVL3.5-1B\noutperforms InternVL3-1B by +2.2% of overall performance. For larger InternVL3.5 variants (such as 38B)\nalso deliver comparable results with other state-of-the-art models of similar scale. Furthermore, InternVL3.5\nexhibits robust generalization on challenging tasks involving long video sequences and complex reasoning, as\nreflected in its performance on LongVideoBench. In particular, InternVL3.5-1B achieves significant improve-\nments on LongVideoBench, i.e.,+4.9%. The model\u2019s ability to process multi-frame inputs and handle diverse\nvideo scenarios underscores its versatility. These results highlight InternVL3.5\u2019s substantial progress in video\nunderstanding, positioning it as a highly capable solution for advanced multimodal video analysis tasks.\n17\n\n--- Page 18 ---\nModel ScreenSpot ScreenSpot-v2 OSWorld-G WindowsAgentArena WebArena-Lite-v2\nShowUI-2B [67] 75.1 \u2013 \u2013 \u2013 \u2013\nUI-TARS-2B [101] 82.3 84.7 \u2013 \u2013 \u2013\nJEDI-3B [154] \u2013 80.9 27.3 \u2013 \u2013\nOS-Atlas-4B [149] 70.1 71.9 \u2013 \u2013 \u2013\nQwen2.5-VL-3B [5] \u2013 80.9 27.3 \u2013 \u2013\nInternVL3.5-4B 83.6 85.1 33.9 9.7 7.8\nOS-Atlas-7B [149] 82.5 84.1 47.5 \u2013 \u2013\nUGround-V1-7B [39] 86.3 \u2013 36.4 \u2013 \u2013\nAguvis-7B [156] 81.8 \u2013 38.7 \u2013 \u2013\nUI-TARS-7B [101] 89.5 91.6 47.5 \u2013 \u2013\nUI-TARS-1.5-7B [101] \u2013 89.7 64.2 15.9 17.5\nJEDI-7B [154] \u2013 91.7 54.1 \u2013 \u2013\nQwen2.5-VL-7B [5] \u2013 88.8 31.4 3.4 \u2013\nGLM-4.1V-9B-Thinking\u2020 [46] \u2013 \u2013 \u2013 \u2013 \u2013\nMiMo-VL-7B-RL [153] 87.2 90.5 50.7 \u2013 \u2013\nInternVL3-8B [186] 79.5 81.4 \u2013 \u2013 \u2013\nInternVL3.5-8B 87.9 86.2 36.4 10.5 12.3\nInternVL3.5-14B 87.5 88.6 44.7 12.5 12.3\nGTA1-32B [161] \u2013 93.2 61.9 \u2013 \u2013\nQwen2.5-VL-32B [5] \u2013 91.3 46.5 \u2013 \u2013\nGemma-3-27B-IT\u2020 [122] \u2013 \u2013 \u2013 \u2013 \u2013\nInternVL3-38B [186] 85.6 88.3 \u2013 \u2013 \u2013\nInternVL3.5-20B-A4B 85.5 87.6 38.2 11.0 9.5\nInternVL3.5-30B-A3B 86.6 87.3 42.4 11.0 10.4\nInternVL3.5-38B 81.0 83.5 42.9 14.5 7.1\nOperator\u2021 [96] \u2013 70.5 40.6 \u2013 \u2013\nAguvis-72B [156] 89.2 \u2013 \u2013 3.5 9.0\nUI-TARS-72B [101] 88.4 90.3 57.1 17.9 10.3\nGPT-4o [97] 18.1 \u2013 \u2013 3.5 1.9\nClaude-3.5-Sonnet [2] 83.0 \u2013 \u2013 \u2013 \u2013\nClaude-3.7-Sonnet [4] \u2013 87.6 \u2013 6.4 1.9\nGemini-2.0-Flash [29] 84.0 \u2013 \u2013 \u2013 \u2013\nGemini-2.5-Pro [28] \u2013 \u2013 45.2 \u2013 \u2013\nSeed1.5-VL [42] \u2013 95.2 62.9 \u2013 \u2013\nQwen2.5-VL-72B [5] 87.1 \u2013 \u2013 9.7 14.4\nInternVL3-78B [186] 88.7 90.9 \u2013 \u2013 \u2013\nInternVL3.5-241B-A28B 89.8 92.9 53.2 18.0 11.7\nTable 10: Comparison of GUI grounding and online agentic evaluation results. To assess the GUI agent\ncapabilities of InternVL3.5, we conducted evaluations on a diverse set of platforms. We evaluate GUI grounding\ncapabilities across 3 benchmarks including ScreenSpot [ 16], ScreenSpot-v2 [ 149] and OSWorld-G [ 154]. For\nonline agentic evaluation, our assessment covers Ubuntu, Windows, and Web utilizing the OSWorld [ 155],\nWindowsAgentArena [ 7], and WebArena-Lite-v2 [ 144]. Models with symbols \u2020and\u2021are evaluated under 100\nand 200 steps, respectively, while all other results were evaluated under 50 steps.\n3.11 GUI Agent Tasks\nTo validate the GUI agent capabilities of InternVL3.5, we conduct detailed experiments in Table 10. In particular,\nwe evaluate InternVL3.5 on six GUI grounding and agent tasks, namely ScreenSpot [ 16], ScreenSpot-v2 [ 149],\nOSWorld-G [ 155], OSWorld [ 155], WindowsAgentArena [ 7], and WebArena-Lite-v2 [ 144]. In GUI grounding\ntasks, InternVL3.5 outperforms most open-source models and is close to the performance of closed-source\nmodels. For example, InternVL3.5-241B-A28B outperforms the specialized model, e.g., +2.6% against UI-\nTARS-72B [ 101] on ScreenSpot-v2. Compared to the most advanced commercial model, i.e.,Seed1.5-VL [ 42],\nInternVL3.5-241B-A28B still maintains close performance on ScreenSpot-v2, i.e.,92.9 vs.95.2. In GUI agent\ntasks, InternVL also demonstrates competitive results on different platforms. In particular, InternVL3.5-241B-\nA28B achieves the best results against existing generalist MLLMs, e.g., +8.3% over Qwen2.5-VL-72B on\nWindowsAgentArena. In particular, GPT-4o only achieves a score of 3.5 on this challenging benchmark. On\nWebArena-Lite-v2, the performance score of GPT-4o further decreases to 1.9, but InternVL3.5-241B-A28B can\nstill achieve a top-tier score of 11.7. These results confirm the great potential of InternVL3.5 as a fundamental\nmodel for GUI tasks.\n18\n\n--- Page 19 ---\nMethod VSI-Bench ERQA SpaCE-10 OmniSpatial Overall\nInternVL3-1B [186] 29.7 30.3 41.3 35.5 34.2\nInternVL3.5-1B 49.3 35.3 33.6 40.7 39.7\nQwen2.5-VL-3B [5] 27.9 38.0 34.8 40.3 35.3\nInternVL3-2B [186] 31.5 31.5 44.2 38.0 36.3\nInternVL3.5-2B 53.8 37.3 34.6 42.3 42.0\nMiniCPM-V-4-4B [163] 30.3 36.3 39.0 43.1 37.2\nInternVL3.5-4B 54.9 38.5 35.5 45.8 43.7\nQwen2.5-VL-7B [5] 35.9 38.8 33.3 39.2 36.8\nMiMo-VL-RL-8B [153] 36.4 37.8 36.1 46.5 39.2\nKeye-VL-8B [126] 28.6 35.3 38.6 46.5 37.2\nGLM-4.1V-9B [46] 39.2 45.8 43.4 47.7 44.0\nInternVL3-8B [186] 42.1 35.3 40.0 41.6 39.7\nInternVL3.5-8B 56.3 41.0 39.5 47.8 46.1\nGemma-3-12B [122] 21.9 36.1 41.5 43.7 35.8\nInternVL3-14B [186] 48.9 39.5 47.3 45.9 45.4\nInternVL3.5-14B 60.8 41.8 48.8 47.6 49.7\nKimi-VL-A3B-2506 [125] 37.4 36.0 39.2 37.3 37.5\nInternVL3.5-20B-A4B 60.1 41.6 51.6 45.4 46.7\nInternVL3.5-30B-A3B 63.7 41.5 49.7 48.1 50.8\nGemma-3-27B [122] 22.0 37.5 41.5 44.8 36.4\nQwen2.5-VL-32B [5] 34.7 40.7 32.6 47.4 38.8\nInternVL3-38B [186] 48.9 42.8 53.1 48.5 48.3\nInternVL3.5-38B 66.3 43.3 43.8 51.4 51.2\nGPT-4o-20241120 [97] 34.0 47.0 49.0 47.8 44.5\nGPT-5-20250807 [98] 37.5 65.7 43.8 59.6 51.7\nGemini-2.5-Pro [29] 47.8 48.3 52.7 55.2 51.0\nClaude-3.7-Sonnet [2] 47.0 35.5 46.2 46.9 43.9\nGLM-4.5V [46] 41.4 46.5 51.6 51.0 47.6\nQwen2.5-VL-72B [5] 36.1 44.8 37.9 47.9 41.7\nStep3-321B-A38B [129] 34.2 44.5 42.6 47.0 42.1\nInternVL3-78B [186] 48.4 45.9 52.5 49.3 49.0\nInternVL3.5-241B-A28B 69.5 46.8 55.0 51.9 55.8\nTable 11: Comparison of embodied task performance. We compare InternVL3.5 and existing methods on\nVSI-Bench [ 160], ERQA [ 121], SpaCE-10 [ 38], and OmniSpatial [ 50]. For SpaCE-10 [ 38], we report the\nsingle-choice performance.\n3.12 Embodied Agent Tasks\nIn Table 11, we evaluate the embodied capabilities of InternVL3.5 on four benchmarks: VSI-Bench [ 160],\nERQA [ 121], Space10 [ 38], and OmniSpatial [ 50]. From this table, we observe the strong embodied capabilities\nof InternVL3.5 against previous works. On VSI-Bench, the most popular benchmark for spatial reasoning,\nInternVL3.5-1B achieves an overall score of 49.3, outperforming its predecessor by +19.6%. We note that\nInternVL3.5-1B can already achieve the state-of-the-art performance on VSI-Bench against much larger models\nlike Qwen2.5-VL-72B [ 5]. When the model size of InternVL3.5 increases, the performance on VSI-Bench\nconsistently improves from 49.3 to 69.5, greatly validating the salability of InternVL3.5 on embodied tasks. In\naddition to VSI-Bench, InternVL3.5 also demonstrates promising results on ERQA, Space10 and OminiSpatial.\nAmong them, InternVL3.5B-241B-A28B scores 46.8 on ERQA, which is close to the 48.3 score of the top\nclosed-source model Gemini-2.5-Pro. In terms of overall performance, InternVL3.5B-241B-A28B also reaches\nthe highest score against other models, confirming its strong capabilities in embodied tasks.\n3.13 SVG Tasks\nScalable vector graphics (SVG) is a common format for describing graphics on web pages, and its under-\nstanding is significant for web-based agent tasks. To evaluate this capability, We evaluate InternVL3.5 on\nSGP-Bench [ 102] (Table 12), where it delivers strong results across model scales and sets new open-source\n19\n\n--- Page 20 ---\nModel Semantics \u2191Count \u2191Color\u2191Shape \u2191Reasoning \u2191 Overall \u2191\nGemma-1.1-2B [123] 32.1 33.3 25.0 35.6 28.7 31.7\nInternVL3.5-1B 25.5 22.7 24.9 24.4 30.4 25.0\nInternVL3.5-2B 25.2 20.1 45.4 33.4 26.5 30.7\nInternVL3.5-4B 41.2 55.2 81.9 62.4 39.4 57.7\nInternLM2.5-7B [9] 27.3 31.7 59.8 51.5 28.2 42.1\nKeye-VL-8B [126] 41.4 47.5 71.4 54.9 40.6 52.2\nGLM-4.1V-9B [46] 41.6 55.6 79.1 61.5 40.0 57.1\nInternVL3-8B [186] 33.7 46.5 69.8 59.1 36.1 50.6\nInternVL3.5-8B 39.7 54.0 82.3 53.4 41.7 54.9\nGemma-3-12B [122] 24.8 30.8 47.2 25.7 22.8 30.5\nDeepSeek-Coder-V2-16B [187] 30.9 37.9 63.7 54.8 26.8 45.1\nInternVL3-14B [186] 38.2 52.9 74.4 54.1 41.7 52.9\nInternVL3.5-14B 44.3 55.3 77.8 63.7 45.1 58.5\nKimi-VL-A3B-2506 [125] 31.1 41.5 67.0 47.4 32.4 44.9\nInternVL3.5-20B-A4B 51.2 60.6 89.8 74.7 55.5 67.6\nInternVL3.5-30B-A3B 51.8 66.8 91.8 75.7 53.8 69.4\nGemma-3-27B [122] 36.7 51.4 76.3 62.1 39.4 54.7\nQwen2.5-VL-32B [5] 40.0 55.7 76.3 61.2 43.9 56.5\nInternVL3-38B [186] 40.8 58.7 82.2 63.6 43.9 59.1\nInternVL3.5-38B 47.6 66.5 90.5 80.4 54.6 69.5\nGPT-5 [98] 67.8 72.6 91.7 81.9 68.7 77.5\nGLM-4.5V [46] 47.3 63.7 87.3 72.3 55.8 66.1\nQwen2.5-VL-72B [5] 40.2 55.1 80.1 62.0 41.1 57.1\nStep3-321B-A38B [129] 35.9 54.0 82.8 63.2 38.6 56.5\nInternVL3-78B [186] 41.0 59.1 84.0 65.2 47.0 60.3\nInternVL3.5-241B-A28B 51.2 69.2 92.1 77.6 58.0 70.7\nTable 12: Comparison of SVG understanding performance on SGP-Bench [102].\nstate-of-the-art at larger capacities. At the small scale, InternVL3.5-4B already surpasses models such as\nKimi-VL-A3B [ 125] and even the earlier InternVL3-14B [ 186]. In the mid-size range (8B, 14B), InternVL3.5\nshows broad improvements, and the 14B variant surpasses larger counterparts such as Gemma-3-27B [ 122]. At\nthe high end, InternVL3.5-30B-A3B and InternVL3.5-38B achieve nearly 70% accuracy on SGP-Bench, advanc-\ning the state-of-the-art among open models and outperforming competitors such as Step3-321B-A38B [ 129],\nQwen2.5-VL-72B [ 5] and GLM-4.5V [ 46]. Finally, InternVL3.5-241B-A28B sets a new record for open-source\nmodels, achieving the best performance across all categories except when compared to GPT-5 [98].\nIn the SArena-Icon generation tasks (Text2SVG and Img2SVG), InternVL3.5 establishes new state-of-the-art\nperformance among open models (Table 13). In Text2SVG, our models achieve markedly lower FID and FID-C\nscores than previous baselines, with the 38B variant reducing FID to 14.56. This performance even surpasses\nGPT-4o [ 97] (15.18), highlighting that InternVL3.5 is able to match or exceed the capabilities of much larger\nproprietary systems. In Img2SVG, the 30B and 38B variants deliver leading results on structural similarity\nmetrics, outperforming open-source peers of comparable scale and closely matching the best proprietary systems.\nFurthermore, InternVL3.5-241B-A28B achieves an even stronger balance of fidelity and perceptual quality, with\nan FID of 11.27 and an FID-C of 4.43 in Text2SVG, together with competitive Img2SVG scores. These results\nhighlight both the efficiency and scalability of InternVL3.5, showing that it consistently outperforms prior open\nmodels across all metrics, narrows the gap with proprietary leaders, and establishes itself as the most powerful\nopen-source framework for SVG generation to date.\n3.14 Evaluation on Language Capability\nTo evaluate the language capabilities of InternVL3.5, we use benchmarks covering comprehensive assessments in\ngeneral knowledge (MMLU [ 44], CMMLU [ 61], C-Eval [ 49], GAOKAO-Bench [ 176]), linguistic understanding\n(TriviaQA [ 52], NaturalQuestions [ 56], C3 [ 115], RACE [ 57]), reasoning (WinoGrande [ 107], HellaSwag [ 171],\nBigBench Hard [ 117]), mathematics (GSM8K-Test [ 18], MATH [ 45], AIME24 [ 84], AIME25 [ 85]), and\n20\n\n--- Page 21 ---\nModelText2SVG Img2SVG\nFID\u2193FID-C \u2193CLIP\u2191 DINO \u2191SSIM\u2191LPIPS \u2193PSNR \u2191\nInternVL3.5-1B 22.50 12.16 72.43 0.79 0.57 0.35 7.35\nInternVL3.5-2B 20.98 11.26 72.71 0.81 0.56 0.34 7.44\nInternVL3.5-4B 17.06 7.54 74.35 0.84 0.61 0.30 8.37\nLlama-3.1-8B [32] 19.43 11.25 71.86 \u2013 \u2013 \u2013 \u2013\nQwen2.5-VL-7B [5] 24.78 15.45 71.38 0.78 0.51 0.38 6.53\nKeye-VL-8B [126] 21.96 14.39 71.17 0.80 0.53 0.37 6.94\nGLM-4.1V-9B [46] 22.68 10.45 73.20 0.82 0.54 0.35 7.33\nInternVL3-8B [186] 23.06 14.30 71.45 0.81 0.56 0.36 7.22\nInternVL3.5-8B 17.36 7.13 75.01 0.85 0.62 0.29 8.74\nLlama-3.2-11B-Vision [32] 28.16 14.35 71.49 0.76 0.47 0.39 5.91\nGemma-3-12B [122] 17.14 10.41 71.62 0.82 0.58 0.35 7.63\nInternVL3-14B [186] 19.00 13.22 71.49 0.83 0.56 0.36 7.34\nInternVL3.5-14B 15.90 5.99 75.91 0.86 0.61 0.31 8.46\nKimi-VL-A3B [125] 30.81 16.99 70.54 0.80 0.56 0.36 7.18\nInternVL3.5-20B-A4B 16.78 5.60 77.46 0.91 0.71 0.20 12.75\nInternVL3.5-30B-A3B 16.31 5.84 76.40 0.88 0.65 0.27 9.64\nGemma-3-27B [122] 15.15 9.30 73.28 0.83 0.60 0.35 7.83\nQwen2.5-VL-32B [5] 20.04 10.39 73.23 0.84 0.56 0.36 7.50\nInternVL3-38B [186] 18.01 11.04 73.08 0.83 0.55 0.35 7.31\nInternVL3.5-38B 14.56 5.22 76.49 0.86 0.61 0.32 8.39\nGrok-3 [150] 21.97 8.69 76.80 \u2013 \u2013 \u2013 \u2013\nLlama-3.1-70B [32] 18.03 8.30 73.88 \u2013 \u2013 \u2013 \u2013\nLlama-3.1-405B [32] 16.79 8.39 73.92 \u2013 \u2013 \u2013 \u2013\nDeepSeek-V3-671B-A37B [68] 24.99 8.80 76.47 \u2013 \u2013 \u2013 \u2013\nGPT-4o [97] 15.18 6.76 77.74 0.87 0.62 0.32 8.44\nGLM-4.5V [46] 16.64 5.09 78.35 0.87 0.63 0.32 8.67\nClaude-3.7-Sonnet [2] 14.38 3.50 80.79 0.91 0.65 0.29 9.26\nClaude-4-Sonnet [3] 15.84 4.29 80.58 0.92 0.67 0.28 9.86\nGemini-2.5-Flash [19] 16.72 5.21 78.22 0.88 0.59 0.32 8.32\nLlama-3.2-90B-Vision [32] 19.31 8.55 74.00 0.76 0.44 0.38 5.78\nLlama-4-Scout [93] 17.91 9.38 73.56 0.84 0.58 0.35 7.74\nLlama-4-Maverick [92] 14.93 6.53 75.82 0.86 0.60 0.33 8.03\nStep3-321B-A38B [129] 20.06 9.71 74.18 0.83 0.56 0.34 7.52\nQwen2.5-VL-72B [5] 15.95 9.88 73.68 0.84 0.58 0.35 7.83\nInternVL3-78B [186] 17.58 10.60 73.12 0.85 0.58 0.34 7.80\nInternVL3.5-241B-A28B 11.27 4.43 76.81 0.88 0.64 0.29 9.19\nTable 13: Comparison of SVG generation performance on SArena-Icon (Text2SVG and Img2SVG).\ncoding (HumanEval [ 12]) tasks. As shown in Table 14, InternVL3.5 achieves even better performance than its\ncorresponding language models on most benchmarks. Specifically, InternVL3.5-1B outperforms Qwen3-0.6B\non 15 of 16 text-related benchmarks, with an overall performance gain of +6.7. For larger models such as\nInternVL3.5-241B-A28B, the performance improvement is also obvious, i.e.,+2.3 over Qwen3-235B-A22B.\nThese improvements come not only from the high-quality text corpora we use during pre-training and SFT,\nbut also from our Cascade RL, which significantly benefits text-based reasoning tasks. The improvement of\nInternVL3.5 in text capabilities has also greatly compensated for the shortcomings of open-source multimodal\nmodels in general capabilities.\n3.15 Ablation Study\nCascade Reinforcement Learning (Cascade RL) . To validate the effectiveness of Cascade RL, we conduct\nan ablation study in Figure 5 and Table 15. We evaluate a baseline model InternVL3 as well as InternVL3.5\n21\n\n--- Page 22 ---\nDataset Version\nQwen3-0.6B\nInternVL3.5-1B\nQwen3-1.7B\nInternVL3.5-2B\nQwen3-4B\nInternVL3.5-4B\nQwen3-8B\nInternVL3.5-8B\nQwen3-14B\nInternVL3.5-14B\nQwen3-30B-A3B\nInternVL3.5-30B-A3B\nQwen3-32B\nInternVL3.5-38B\nQwen3-235B-A22B\nInternVL3.5-241B-A28B\nMMLU 4d595a 52.8 49.1 62.6 61.9 73.0 73.8 76.9 78.2 81.1 81.5 81.4 83.0 83.6 84.6 87.8 89.1\nCMMLU c13365 43.4 46.6 59.8 59.4 71.8 70.6 76.6 76.2 81.6 79.8 83.0 82.5 84.5 84.4 87.4 90.2\nC-Eval 2daf24 42.6 49.0 61.0 61.0 72.2 71.9 77.9 77.9 81.0 80.3 82.9 83.2 83.3 85.0 86.1 90.9\nGAOKAO 4c31db 40.4 48.4 64.1 68.1 80.0 83.2 85.6 84.1 90.0 87.2 89.5 92.6 93.2 93.4 95.0 94.5\nTriviaQA 2121ce 18.7 20.4 30.6 31.7 39.9 40.2 52.0 49.7 60.9 55.3 58.4 57.6 63.4 59.9 73.7 74.8\nNaturalQuestions 3dcea1 11.2 15.0 21.4 23.5 29.3 29.0 36.5 32.6 42.2 36.7 42.5 36.7 45.8 40.1 54.8 50.4\nC3 8c358f 64.1 65.5 54.2 79.5 79.8 89.5 83.0 91.6 84.3 94.6 90.8 95.1 93.5 95.6 96.4 97.8\nRACE-High 69ee4f 45.3 67.2 65.6 78.8 82.7 86.4 86.2 88.4 87.2 90.7 78.7 92.3 85.6 92.1 90.5 94.2\nWinoGrande b36770 51.5 54.9 53.5 59.3 64.8 69.1 70.8 75.2 76.6 80.5 73.0 86.5 74.8 83.4 84.8 91.7\nHellaSwag e42710 42.7 49.5 59.0 74.4 78.1 88.5 84.6 91.5 88.2 94.1 89.0 96.3 90.5 95.9 91.1 97.0\nBBH 5b92b0 41.5 46.8 54.5 62.3 72.6 78.6 78.4 79.8 81.1 82.3 81.5 83.8 87.4 87.5 88.9 86.5\nGSM8K 1d7fe4 59.6 62.8 75.4 77.2 87.8 92.0 89.8 90.9 92.5 95.6 91.8 91.4 93.4 91.5 94.4 91.6\nMATH 393424 32.4 68.2 43.5 85.5 54.1 94.4 60.8 93.3 62.0 93.6 59.0 93.6 61.6 94.3 71.8 94.5\nAIME2024 \u2013 10.0 13.7 40.0 44.2 66.7 72.8 76.0 77.7 80.0 77.4 83.3 79.4 73.3 81.5 86.7 84.1\nAIME2025 \u2013 13.3 14.7 23.3 42.9 50.0 57.6 67.3 64.0 66.7 63.9 70.0 62.7 60.0 72.1 83.3 75.6\nHumanEval 8e312c 39.6 45.7 72.0 65.9 82.3 87.8 85.4 93.9 89.0 97.0 89.0 96.3 89.6 98.2 91.5 98.2\nOverall \u2013 38.1 44.8 52.5 61.0 67.8 74.1 74.6 77.8 77.8 80.7 77.8 82.0 79.0 83.7 85.3 87.6\nTable 14: Comparison of text-related performance across multiple benchmarks. Results were obtained\nusing the OpenCompass toolkit [ 20]. We compare InternVL3.5 with Qwen3 models, whose corresponding\npre-trained base models are employed as the initialization of the language component in InternVL3.5. Please\nnote that the evaluation scores of the Qwen3 series may differ from those officially reported, as we have adopted\nthe prompt versions provided in the table across all datasets for OpenCompass evaluation.\n1B 2B 4B 8B 14B 30B-A3B 38B 241B-A28B25.1 25.729.133.832.438.541.150.749.452.357.4\n44.353.656.360.3\n49.654.956.662.0\n52.756.159.0\n52.757.761.466.0\n60.462.466.9InternVL3\nInternVL3.5-Instruct\nInternVL3.5-MPO\nInternVL3.5-CascadeRL\nFigure 5: Ablation on Cascade RL. We report average scores on the same set of multimodal reasoning and\nmathematical benchmarks as in Table 3. Full results are provided in Table 15.\nafter different training stages: InternVL3.5-Instruct (after SFT), InternVL3.5-MPO (after the first substage\nin Cascade RL), and InternVL3.5-CascadeRL (after both substages in Cascade RL). From it we can see that\nInternVL3.5-Instruct already outperforms InternVL3 by margins, e.g., + 9.3% on the 8B model. Even based\non these strong SFT baselines, the performance of InternVL3.5 can be further improved after the MPO stage,\nproviding up to +3.5% average gains on reasoning tasks. Compared to MPO-based models, our Cascade RL still\nprovides orthogonal gains for all dense and MoE models. For example, InternVL3.5-2B obtains 12.2% average\nperformance gains on reasoning tasks compared to the SFT model. Similar merits can also be observed on\nlarger models, e.g., +6.5% on InternVL3.5-241B-A28B. These ablations confirm the effectiveness, stability, and\nscalability of our Cascade RL. Additionally, we present a comparison of the efficiency and effectiveness of our\nproposed Cascade RL against MPO and GSPO in Table 16. For MPO and Cascade RL, we report performance\nafter training for one episode, whereas for GSPO, we report results after both one and two episodes. We observe\nthat MPO achieves performance gains with only a small number of GPU hours, while GSPO yields more\nsignificant improvements but at the cost of substantially higher computational consumption. In contrast, Cascade\nRL attains even greater performance improvements while requiring only half the GPU hours of GSPO.\n22\n\n--- Page 23 ---\nModelMMMU\n(val)MathVista\n(mini)MathVisionMathVerse\n(vision-only)DynaMath\n(worst case)WeMath LogicVista Overall\nInternVL3-1B 43.4 45.8 18.8 18.7 5.8 13.4 29.8 25.1\nInternVL3.5-1B-Instruct 37.2 48.6 15.8 27.0 8.4 13.9 29.1 25.7\nInternVL3.5-1B-MPO 40.3 50.5 22.0 32.1 9.0 16.8 32.7 29.1\nInternVL3.5-1B-CascadeRL 44.2 59.3 27.3 37.8 17.2 21.5 29.3 33.8\nInternVL3-2B 48.6 57.0 21.7 25.3 14.6 22.4 36.9 32.4\nInternVL3.5-2B-Instruct 53.0 60.8 27.0 39.6 19.8 28.1 41.2 38.5\nInternVL3.5-2B-MPO 54.3 62.6 34.2 46.4 21.0 28.1 40.9 41.1\nInternVL3.5-2B-CascadeRL 59.0 71.8 42.8 53.4 31.5 48.5 47.7 50.7\nInternVL3.5-4B-Instruct 64.3 71.4 40.5 50.0 30.7 35.6 53.5 49.4\nInternVL3.5-4B-MPO 65.4 71.7 48.0 54.9 30.7 39.8 55.9 52.3\nInternVL3.5-4B-CascadeRL 66.6 77.1 54.4 61.7 35.7 50.1 56.4 57.4\nInternVL3-8B 62.7 71.6 29.3 39.8 25.5 37.1 44.1 44.3\nInternVL3.5-8B-Instruct 68.1 74.2 46.4 55.8 30.7 46.0 53.9 53.6\nInternVL3.5-8B-MPO 71.2 75.9 52.6 54.8 33.1 47.7 58.6 56.3\nInternVL3.5-8B-CascadeRL 73.4 78.4 56.8 61.5 37.7 57.0 57.3 60.3\nInternVL3-14B 67.1 75.1 37.2 44.4 31.3 43.0 51.2 49.9\nInternVL3.5-14B-Instruct 71.8 73.4 48.7 55.5 31.9 45.7 57.5 54.9\nInternVL3.5-14B-MPO 73.3 74.0 53.0 57.5 32.3 45.2 60.9 56.6\nInternVL3.5-14B-CascadeRL 73.3 80.5 59.9 62.8 38.7 58.7 60.2 62.0\nInternVL3.5-30B-A3B-Instruct 72.3 73.3 45.1 50.4 31.9 39.7 56.4 52.7\nInternVL3.5-30B-A3B-MPO 71.7 75.3 50.7 58.5 32.9 43.7 59.7 56.1\nInternVL3.5-30B-A3B-CascadeRL 75.6 80.9 55.7 60.4 36.5 48.4 55.7 59.0\nInternVL3-38B 70.1 75.1 34.2 48.2 35.3 48.6 58.4 52.8\nInternVL3.5-38B-Instruct 73.9 75.9 58.2 59.0 29.7 47.5 60.0 57.7\nInternVL3.5-38B-MPO 76.9 80.5 56.3 59.4 36.9 55.6 64.2 61.4\nInternVL3.5-38B-CascadeRL 76.9 81.9 63.7 67.6 41.7 64.8 65.3 66.0\nInternVL3-78B 72.2 79.0 43.1 51.0 35.1 46.1 55.9 54.6\nInternVL3.5-241B-A28B-Instruct 76.2 80.1 55.6 61.7 36.5 49.7 63.3 60.4\nInternVL3.5-241B-A28B-MPO 76.0 82.2 55.3 64.1 38.3 51.3 69.4 62.4\nInternVL3.5-241B-A28B-CascadeRL 77.7 82.7 63.9 68.5 46.5 62.3 66.7 66.9\nTable 15: Comparison of multimodal reasoning performance after different training stages.\nModel GPU HoursMMMU\nValMathVista\nMINIMathVision\nMINIMathVerse\nVision-OnlyDynaMath\n(Worst)WeMath LogicVista Overall\nInternVL3.5-8B-Instruct \u2013 68.1 74.2 46.4 55.8 30.7 46.0 53.9 53.6\n+MPO \u223c0.3K Hours 71.2 75.9 52.6 54.8 33.1 47.7 58.6 56.3\n+GSPO (1 episode) \u223c5.5K Hours 73.8 77.9 51.6 58.8 35.1 48.9 54.8 57.3\n+GSPO (2 episodes) \u223c11.0K Hours 72.0 78.1 51.6 58.5 35.7 54.1 57.0 58.2\n+CascadeRL (ours) \u223c5.8K Hours 73.4 78.4 56.8 61.5 37.7 57.0 57.3 60.3\nTable 16: Comparison of training efficiency and effectiveness of MPO, GSPO, and Cascade RL.\nVisual Resolution Router (ViR) . In Tables 17 and 18, we compare efficiency and performance of InternVL3.5\nwith and without ViR, and models equipped with ViR are called InternVL3.5-Flash. In Table 18 , we validate\nthe efficiency improvement brought by ViR. From it we can see that the proposed DvD can already accelerate\ninference by up to 2.01 \u00d7, based on which ViR still provides significant efficiency gains, e.g., 4.05\u00d7speedup.\nNote that the efficiency gains of ViR are also obvious for the large MoE model, i.e.,InternVL3.5-241B-A28B,\nwhich is significant for real-world application.\nIn Table 17, we compare the performance of InternVL3.5 and InternVL3.5-Flash on several significant bench-\nmarks. For these results, we can see that InternVL3.5-Flash can maintain the multimodal understanding and\nreasoning performance. In high-resolution tasks like DocVQA and InfoVQA, InternVL3.5-Flash can reach\nalmost the 100% performance of InternVL3.5, e.g., 80.2 vs.79.8 on 8B model size. Even when the model size\nimproves to 241B, similar observations still hold. These results further confirm that ViR can greatly benefit the\nmodel performance without sacrificing performance.\nDecoupled Vision-Language Deployment (DvD) . In Table 18, we conduct detailed ablation on InternVL3.5 to\nshow the benefit of DvD. From this table, the first observation is that DvD greatly accelerates the inference of\nboth dense and MoE models, by up to 2.01 times and 1.97 times for InternVL3.5-241B-A28B and InternVL3.5-\n38B, respectively. In addition, the efficiency gains of DvD can benefit both the pre-filling and next-token\n23\n\n--- Page 24 ---\nModel DocVQA ChartVQA InfoVQA TextVQA OCRBench AI2D MMStar MMMU Mathvista Overall\nInternVL3.5-8B 92.3 86.7 76.2 78.2 83.2 84.0 69.3 73.4 78.4 80.2\nInternVL3.5-8B-Flash 91.9 86.6 76.0 77.2 83.0 83.6 68.6 72.9 78.0 79.8\nInternVL3.5-38B 94.0 88.8 81.0 82.7 87.0 87.8 75.3 76.9 81.9 83.9\nInternVL3.5-38B-Flash 93.5 88.1 81.0 82.1 86.5 87.2 75.0 76.3 81.3 83.4\nInternVL3.5-30B-MoE 94.2 87.4 77.8 80.5 88.0 86.8 72.0 75.6 80.9 82.6\nInternVL3.5-30B-MoE-Flash 93.2 87.3 77.6 80.2 87.8 86.3 71.7 75.2 80.5 82.2\nInternVL3.5-235B-MoE 94.9 88.0 82.0 84.5 90.7 86.9 77.9 77.7 82.7 85.0\nInternVL3.5-235B-MoE-Flash 94.0 87.9 81.0 84.1 90.3 86.1 77.4 77.0 83.0 84.5\nTable 17: Performance comparison of InternVL3.5 and InternVL3.5-Flash.\nResolution SettingRequest Throughput (requests / s)\nInternVL3.5-38B InternVL3.5-241B-A28B\n448Baseline 12.39 11.29\n+ DvD 14.69 (1.19 \u00d7) 14.05 (1.24 \u00d7)\n+ DvD + ViR 18.62 (1.50 \u00d7) 20.84 (1.85 \u00d7)\n896Baseline 2.71 2.54\n+ DvD 5.06 (1.87 \u00d7) 4.73 (1.86 \u00d7)\n+ DvD + ViR 10.97 (4.05 \u00d7) 8.81 (3.47 \u00d7)\n1344Baseline 1.48 1.37\n+ DvD 2.92 (1.97 \u00d7) 2.75 (2.01 \u00d7)\n+ DvD + ViR 5.14 (3.47 \u00d7) 4.27 (3.12 \u00d7)\nTable 18: Ablation of Decoupled Vision-Language Deployment (DvD) and Visual Resolution Router (ViR)\non inference efficiency. We send 16 requests per second to the deployed server. In all settings, the language\nmodels are deployed on 8 A100 GPUs.\nprediction stages of InternVL3.5. Another finding is that as the input resolution increases, DVD efficiency also\nimproves. For example, on InternVL3.5-38B, the speed-up of DvD can be improved from 1.19 to 1.97 as the\nresolution increases from 448 to 1344. This phenomenon can be attributed to the fact that larger input resolution\nor visual backbone leads to higher visual computational costs and further blocks the computation of the LLM.\nIt is worth noting that the increased computational cost of high-resolution images arises because mainstream\nMLLMs typically adopt a dynamic high-resolution strategy, which increases the number of patches fed into the\nvision encoder and thereby raises the overall computation. In practical applications, beyond high-resolution\nimages, many tasks also require multi-image and video understanding. In such scenarios, the number of patches\nprocessed by the vision encoder grows even further, leading to greater visual overhead. We believe that our\nproposed DvD can deliver even more significant performance gains in these scenarios.\n4 Conclusion\nIn this work, we introduce InternVL3.5, the latest family of InternVL models that demonstrates stronger\ngeneral performance and faster speed across a wide range of tasks. InternVL3.5 adopts a new reinforcement\nlearning (RL) framework, namely Cascade RL, which combines the benefits of offline and online RL methods\nto boost reasoning capabilities. In addition, two techniques are further introduced to reduce the inference cost\nof InternVL3.5, namely Visual Resolution Router (ViR) and Decoupled Vision-Language Deployment (DvD).\nBenefiting from these innovations, InternVL3.5 achieves +16.0% improvement in overall reasoning performance\nand 4.05 \u00d7speed-up in inference efficiency compared to InternVL3. Besides, InternVL3.5 has significant\nimprovements in its versatility against InternVL3. Specifically, InternVL3.5-241B-A28B achieves the highest\noverall score across multimodal general, reasoning, text, and agency tasks among leading open-source MLLMs,\nsignificantly narrowing the performance gap with top-tier commercial models like GPT-5. We believe that our\nopen source models and codes will push forward multimodal AI research and its real-world applications.\n24\n\n--- Page 25 ---\nReferences\n[1]Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 ,\n2023. 17\n[2]Anthropic. The claude 3 model family: Opus, sonnet, haiku. https://www.anthropic.com , 2024. 11, 12, 13,\n14, 18, 19, 21\n[3] Anthropic. Introducing claude 4: Claude sonnet 4 and claude opus 4, May 2025. 21\n[4] Sonnet Anthropic. Claude 3.7 sonnet system card. 2025. 18\n[5]Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun\nTang, et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923 , 2025. 2, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n19, 20, 21\n[6]Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von Werra. Smollm-corpus, July\n2024. 4\n[7]Rogerio Bonatti, Dan Zhao, Francesco Bonacci, Dillon Dupont, Sara Abdali, Yinheng Li, Yadong Lu, Justin Wagle,\nKazuhito Koishida, Arthur Bucker, et al. Windows agent arena: Evaluating multi-modal os agents at scale. arXiv\npreprint arXiv:2409.08264 , 2024. 18\n[8]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural\ninformation processing systems , 33:1877\u20131901, 2020. 5\n[9]Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei\nChu, et al. Internlm2 technical report. arXiv preprint arXiv:2403.17297 , 2024. 4, 20\n[10] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal\nllm\u2019s referential dialogue magic. arXiv preprint arXiv:2306.15195 , 2023. 15\n[11] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao,\nDahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint\narXiv:2403.20330 , 2024. 2, 8, 14\n[12] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri\nEdwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code.\narXiv preprint arXiv:2107.03374 , 2021. 1, 21\n[13] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian,\nZhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and\ntest-time scaling. arXiv preprint arXiv:2412.05271 , 2024. 2, 3, 4, 5, 7, 11\n[14] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng\nLuo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source\nsuites. arXiv preprint arXiv:2404.16821 , 2024. 2, 3\n[15] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu,\nLewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 24185\u201324198, 2024.\n2, 3\n[16] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick:\nHarnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935 , 2024. 10, 18\n[17] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang\nLuo, Deli Zhao, et al. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms.\narXiv preprint arXiv:2406.07476 , 2024. 17\n[18] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert,\nJerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint\narXiv:2110.14168 , 2021. 20\n[19] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel\nBlistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning,\nmultimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261 , 2025. 21\n25\n\n--- Page 26 ---\n[20] OpenCompass Contributors. Opencompass: A universal evaluation platform for foundation models. https:\n//github.com/open-compass/opencompass , 2023. 9, 11, 12, 13, 14, 22\n[21] XTuner Contributors. Xtuner: A toolkit for efficiently fine-tuning llm. https://github.com/InternLM/\nxtuner , 2023. 7\n[22] X.AI Corp. Grok-1.5 vision preview: Connecting the digital and physical worlds with our first multimodal model.\nhttps://x.ai/blog/grok-1.5v , 2024. 13\n[23] Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu,\nWeize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456 , 2025. 1\n[24] Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen,\nWeize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint\narXiv:2505.22617 , 2025. 1\n[25] Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Moham-\nmad Shoeybi, Bryan Catanzaro, and Wei Ping. Nvlm: Open frontier-class multimodal llms. arXiv preprint\narXiv:2409.11402 , 2024. 12\n[26] Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference\non Learning Representations (ICLR) , 2024. 7\n[27] Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. FlashAttention: Fast and memory-efficient\nexact attention with IO-awareness. In Advances in Neural Information Processing Systems (NeurIPS) , 2022. 7\n[28] DeepMind. Gemini 2.5 pro. https://deepmind.google/technologies/gemini/pro/ , 2025. 18\n[29] Google Deepmind. Introducing gemini 2.0: our new ai model for the agentic era. https://blog.google/\ntechnology/google-deepmind/google-gemini-ai-update-december-2024/ , 2024. 11, 18, 19\n[30] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi,\nNiklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art\nmultimodal models. arXiv preprint arXiv:2409.17146 , 2024. 12\n[31] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan\nZhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In\nProceedings of the 32nd ACM International Conference on Multimedia , pages 11198\u201311201, 2024. 8, 9\n[32] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil\nMathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 ,\n2024. 12, 21\n[33] Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. Mmbench-video: A\nlong-form multi-shot benchmark for holistic video understanding. arXiv preprint arXiv:2406.14515 , 2024. 17\n[34] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang,\nXiawu Zheng, et al. Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv\npreprint arXiv:2306.13394 , 2023. 14\n[35] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang\nShen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in\nvideo analysis. arXiv preprint arXiv:2405.21075 , 2024. 2, 8, 17\n[36] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A Smith, Wei-Chiu\nMa, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. arXiv preprint\narXiv:2404.12390 , 2024. 2, 12, 13\n[37] Zhangwei Gao, Zhe Chen, Erfei Cui, Yiming Ren, Weiyun Wang, Jinguo Zhu, Hao Tian, Shenglong Ye, Junjun\nHe, Xizhou Zhu, et al. Mini-internvl: A flexible-transfer pocket multimodal model with 5% parameters and 90%\nperformance. arXiv preprint arXiv:2410.16261 , 2024. 2, 3\n[38] Ziyang Gong, Wenhao Li, Oliver Ma, Songyuan Li, Jiayi Ji, Xue Yang, Gen Luo, Junchi Yan, and Rongrong Ji.\nSpace-10: A comprehensive benchmark for multimodal large language models in compositional spatial intelligence.\narXiv preprint arXiv:2506.07966 , 2025. 1, 2, 10, 19\n[39] Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating\nthe digital world as humans do: Universal visual grounding for GUI agents. In The Thirteenth International Conference\non Learning Representations , 2025. 18\n26\n\n--- Page 27 ---\n[40] Shuhao Gu, Jialing Zhang, Siyuan Zhou, Kevin Yu, Zhaohu Xing, Liangdong Wang, Zhou Cao, Jintao Jia, Zhuoyi\nZhang, Yixuan Wang, et al. Infinity-mm: Scaling multimodal performance with large-scale and high-quality instruction\ndata. arXiv preprint arXiv:2410.18558 , 2024. 12\n[41] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong\nHuang, Yaser Yacoob, et al. Hallusionbench: An advanced diagnostic suite for entangled language hallucination &\nvisual illusion in large vision-language models. arXiv preprint arXiv:2310.14566 , 2023. 2, 8, 14, 15\n[42] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang,\nJiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062 , 2025. 11, 18\n[43] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang,\nYuxiang Zhang, et al. Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual\nmultimodal scientific problems. arXiv preprint arXiv:2402.14008 , 2024. 1, 8\n[44] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\nMeasuring massive multitask language understanding. In The International Conference on Learning Representations ,\n2020. 1, 20\n[45] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob\nSteinhardt. Measuring mathematical problem solving with the MATH dataset. In Joaquin Vanschoren and Sai-Kit\nYeung, editors, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1,\nNeurIPS Datasets and Benchmarks 2021, December 2021, virtual , 2021. 2, 20\n[46] Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji,\nLihang Pan, et al. Glm-4.1 v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning.\narXiv preprint arXiv:2507.01006 , 2025. 1, 3, 9, 10, 11, 12, 13, 14, 16, 18, 19, 20, 21\n[47] Pin-Lun Hsu, Yun Dai, Vignesh Kothapalli, Qingquan Song, Shao Tang, Siyu Zhu, Steven Shimizu, Shivam Sahni,\nHaowen Ning, Yanning Chen, and Zhipeng Wang. Liger-kernel: Efficient triton kernels for llm training. In\nChampioning Open-source DEvelopment in ML Workshop @ ICML25 , 2025. 7\n[48] Xueyu Hu, Tao Xiong, Biao Yi, Zishu Wei, Ruixuan Xiao, Yurun Chen, Jiasheng Ye, Meiling Tao, Xiangxin Zhou,\nZiyu Zhao, et al. Os agents: A survey on mllm-based agents for general computing devices use. arXiv preprint\narXiv:2508.04482 , 2025. 1\n[49] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv,\nYikai Zhang, Yao Fu, et al. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models.\nAdvances in Neural Information Processing Systems , 36, 2024. 1, 20\n[50] Mengdi Jia, Zekun Qi, Shaochen Zhang, Wenyao Zhang, Xinqiang Yu, Jiawei He, He Wang, and Li Yi. Omnispatial:\nTowards comprehensive spatial reasoning benchmark for vision language models. arXiv preprint arXiv:2506.03135 ,\n2025. 1, 2, 19\n[51] Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Liu, and Wenhu Chen. Mantis: Interleaved\nmulti-image instruction tuning. arXiv preprint arXiv:2405.01483 , 2024. 12, 13\n[52] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised\nchallenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551 , 2017. 20\n[53] Seungjae Jung, Gunsoo Han, Daniel Wontae Nam, and Kyoung-Woon On. Binary classifier optimization for large\nlanguage model alignment. arXiv preprint arXiv:2404.04656 , 2024. 5\n[54] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in\nphotographs of natural scenes. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language\nProcessing , pages 787\u2013798, 2014. 15\n[55] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. A diagram is\nworth a dozen images. In European Conference on Computer Vision , pages 235\u2013251, 2016. 2, 8, 11, 12\n[56] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle\nEpstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering\nresearch. Transactions of the Association for Computational Linguistics , 7:453\u2013466, 2019. 20\n[57] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading comprehension\ndataset from examinations. arXiv preprint arXiv:1704.04683 , 2017. 20\n[58] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and\nChunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326 , 2024. 17\n27\n\n--- Page 28 ---\n[59] Bohao Li, Yuying Ge, Yi Chen, Yixiao Ge, Ruimao Zhang, and Ying Shan. Seed-bench-2-plus: Benchmarking\nmultimodal large language models with text-rich visual comprehension. arXiv preprint arXiv:2404.16790 , 2024. 11,\n12\n[60] Chunyi Li, Jianbo Zhang, Zicheng Zhang, Haoning Wu, Yuan Tian, Wei Sun, Guo Lu, Xiaohong Liu, Xiongkuo\nMin, Weisi Lin, et al. R-bench: Are your large multimodal model robust to real-world corruptions? arXiv preprint\narXiv:2410.05474 , 2024. 13\n[61] Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. Cmmlu:\nMeasuring massive multitask language understanding in chinese. arXiv preprint arXiv:2306.09212 , 2023. 10, 20\n[62] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao.\nVideochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355 , 2023. 17\n[63] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo,\net al. Mvbench: A comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pages 22195\u201322206, 2024. 2, 8, 17\n[64] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination\nin large vision-language models. In The Conference on Empirical Methods in Natural Language Processing , pages\n292\u2013305, 2023. 14, 15\n[65] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John\nSchulman, Ilya Sutskever, and Karl Cobbe. Let\u2019s verify step by step. In The Twelfth International Conference on\nLearning Representations , 2023. 7, 8\n[66] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for\nvisual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,\npages 26689\u201326699, 2024. 12, 17\n[67] Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Shiwei Wu, Zechen Bai, Weixian Lei, Lijuan Wang, and\nMike Zheng Shou. Showui: One vision-language-action model for gui visual agent, 2024. 18\n[68] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu\nZhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437 , 2024. 7, 21\n[69] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang,\nHang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European\nConference on Computer Vision , pages 38\u201355. Springer, 2025. 15\n[70] Wentao Liu, Qianjun Pan, Yi Zhang, Zhuo Liu, Ji Wu, Jie Zhou, Aimin Zhou, Qin Chen, Bo Jiang, and Liang\nHe. Cmm-math: A chinese multimodal math dataset to evaluate and enhance the mathematics reasoning of large\nmultimodal models. arXiv preprint arXiv:2409.02834 , 2024. 7\n[71] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui\nHe, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281 ,\n2023. 2, 8, 14\n[72] Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui Chen,\nChunyuan Li, Lianwen Jin, et al. On the hidden mystery of ocr in large multimodal models. arXiv preprint\narXiv:2305.07895 , 2023. 2, 8, 11, 12\n[73] Zihan Liu, Yang Chen, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Acemath: Advancing frontier math\nreasoning with post-training and reward modeling. arXiv preprint , 2024. 4\n[74] Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Oryx mllm: On-demand spatial-\ntemporal understanding at arbitrary resolution. arXiv preprint arXiv:2409.12961 , 2024. 17\n[75] Dakuan Lu, Xiaoyu Tan, Rui Xu, Tianchu Yao, Chao Qu, Wei Chu, Yinghui Xu, and Yuan Qi. Scp-116k: A\nhigh-quality problem-solution dataset and a generalized pipeline for automated extraction in the higher education\nscience domain, 2025. 4\n[76] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang,\nMichel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual\ncontexts. arXiv preprint arXiv:2310.02255 , 2023. 1, 2, 8, 10, 11\n[77] Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Han-Jia Ye. Ovis: Structural\nembedding alignment for multimodal large language model. arXiv preprint arXiv:2405.20797 , 2024. 11, 12\n28\n",
  "project_dir": "artifacts/projects/enhanced_cs.CV_2508.18265v1_InternVL35_Advancing_Open_Source_Multimodal_Mode",
  "communication_dir": "artifacts/projects/enhanced_cs.CV_2508.18265v1_InternVL35_Advancing_Open_Source_Multimodal_Mode/.agent_comm",
  "assigned_at": "2025-08-26T21:09:48.378426",
  "status": "assigned"
}